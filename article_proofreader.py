#!/usr/bin/env python3
"""
è¨˜äº‹æ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ 
ãƒ—ãƒ­ã®ãƒ©ã‚¤ã‚¿ãƒ¼è¦–ç‚¹ã§è¨˜äº‹ã‚’æ ¡æ­£ã—ã€æ­£ç¢ºæ€§ã¨å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹
"""

import json
import re
import asyncio
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Tuple
import httpx
from urllib.parse import urlparse

# æ—¥æœ¬æ¨™æº–æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³
JST = timezone(timedelta(hours=9))

class ArticleProofreader:
    """è¨˜äº‹ã®æ ¡æ­£ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.proofreading_rules_file = Path("BLOG_PROOFREADING_RULES.md")
        self.proofreading_log_file = Path("proofreading_log.json")
        self.load_proofreading_log()
        
    def load_proofreading_log(self):
        """æ ¡æ­£ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã‚€"""
        if self.proofreading_log_file.exists():
            with open(self.proofreading_log_file, "r", encoding="utf-8") as f:
                self.proofreading_log = json.load(f)
        else:
            self.proofreading_log = {
                "logs": [],
                "common_issues": {},
                "improvement_patterns": []
            }
    
    def save_proofreading_log(self):
        """æ ¡æ­£ãƒ­ã‚°ã‚’ä¿å­˜"""
        with open(self.proofreading_log_file, "w", encoding="utf-8") as f:
            json.dump(self.proofreading_log, f, ensure_ascii=False, indent=2)
    
    async def proofread_article(self, article_path: Path) -> Dict[str, Any]:
        """è¨˜äº‹ã‚’æ ¡æ­£ã™ã‚‹"""
        
        with open(article_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        metadata = self._extract_metadata(content)
        
        # æ ¡æ­£çµæœã‚’åˆæœŸåŒ–
        proofreading_result = {
            "article_id": article_path.stem,
            "timestamp": datetime.now(JST).isoformat(),
            "original_score": 100,  # æ¸›ç‚¹æ–¹å¼
            "issues_found": [],
            "corrections": [],
            "final_score": 100
        }
        
        # å„é …ç›®ã‚’ãƒã‚§ãƒƒã‚¯
        technical_issues = await self._check_technical_accuracy(content, metadata)
        timeliness_issues = await self._check_timeliness(content, metadata)
        quality_issues = self._check_writing_quality(content)
        practicality_issues = self._check_practicality(content)
        
        # ã™ã¹ã¦ã®å•é¡Œã‚’çµ±åˆ
        all_issues = technical_issues + timeliness_issues + quality_issues + practicality_issues
        proofreading_result["issues_found"] = all_issues
        
        # ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        for issue in all_issues:
            if issue["severity"] == "high":
                proofreading_result["original_score"] -= 10
            elif issue["severity"] == "medium":
                proofreading_result["original_score"] -= 5
            else:
                proofreading_result["original_score"] -= 2
        
        # è‡ªå‹•ä¿®æ­£ã‚’é©ç”¨
        corrected_content = content
        for issue in all_issues:
            if issue.get("auto_correctable", False):
                corrected_content, correction = self._apply_correction(
                    corrected_content, issue
                )
                if correction:
                    proofreading_result["corrections"].append(correction)
        
        # æœ€çµ‚ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        proofreading_result["final_score"] = min(
            100,
            proofreading_result["original_score"] + len(proofreading_result["corrections"]) * 3
        )
        
        # ä¿®æ­£ãŒå¿…è¦ãªå ´åˆã¯è¨˜äº‹ã‚’æ›´æ–°
        if proofreading_result["corrections"]:
            proofreading_result["corrected_content"] = corrected_content
            proofreading_result["auto_corrected"] = True
        else:
            proofreading_result["auto_corrected"] = False
        
        return proofreading_result
    
    def _extract_metadata(self, content: str) -> Dict[str, str]:
        """è¨˜äº‹ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        metadata = {}
        
        if content.startswith("---"):
            parts = content.split("---", 2)
            if len(parts) >= 2:
                meta_section = parts[1].strip()
                for line in meta_section.split("\n"):
                    if ":" in line:
                        key, value = line.split(":", 1)
                        metadata[key.strip()] = value.strip()
        
        return metadata
    
    async def _check_technical_accuracy(self, content: str, metadata: Dict[str, str]) -> List[Dict[str, Any]]:
        """æŠ€è¡“çš„æ­£ç¢ºæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯
        version_patterns = [
            (r'Python (\d+\.\d+(?:\.\d+)?)', 'Python', '3.11'),
            (r'Node\.js (\d+(?:\.\d+)?)', 'Node.js', '20'),
            (r'React (\d+(?:\.\d+)?)', 'React', '18.2'),
            (r'Next\.js (\d+(?:\.\d+)?)', 'Next.js', '15'),
            (r'Vue (\d+(?:\.\d+)?)', 'Vue', '3.4'),
        ]
        
        for pattern, tech, latest in version_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if self._is_version_outdated(match, latest):
                    issues.append({
                        "type": "version_outdated",
                        "severity": "medium",
                        "location": f"{tech} {match}",
                        "original": match,
                        "suggestion": latest,
                        "auto_correctable": True,
                        "pattern": f"{tech} {match}",
                        "replacement": f"{tech} {latest}"
                    })
        
        # éæ¨å¥¨ã®ç”¨èªã‚„æ‰‹æ³•ã‚’ãƒã‚§ãƒƒã‚¯
        deprecated_terms = {
            "componentWillMount": "useEffect",
            "componentWillReceiveProps": "useEffect or getDerivedStateFromProps",
            "findDOMNode": "ref",
            "React.createClass": "class components or function components",
            "var ": "let or const",
        }
        
        for deprecated, replacement in deprecated_terms.items():
            if deprecated in content:
                issues.append({
                    "type": "deprecated_usage",
                    "severity": "high",
                    "location": f"Usage of {deprecated}",
                    "original": deprecated,
                    "suggestion": replacement,
                    "auto_correctable": deprecated == "var ",
                    "pattern": deprecated,
                    "replacement": "const " if deprecated == "var " else None
                })
        
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®æ–‡æ³•ã‚’ãƒã‚§ãƒƒã‚¯
        code_blocks = re.findall(r'```(?:python|javascript|typescript)?\n(.*?)\n```', content, re.DOTALL)
        for i, code in enumerate(code_blocks):
            syntax_issues = self._check_code_syntax(code)
            for issue in syntax_issues:
                issue["location"] = f"Code block {i+1}"
                issues.append(issue)
        
        # URLã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        urls = re.findall(r'https?://[^\s\)]+', content)
        for url in urls[:5]:  # æœ€åˆã®5å€‹ã®ã¿ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãŸã‚ï¼‰
            if not await self._is_url_valid(url):
                issues.append({
                    "type": "invalid_url",
                    "severity": "low",
                    "location": url,
                    "original": url,
                    "suggestion": "Check URL validity",
                    "auto_correctable": False
                })
        
        return issues
    
    async def _check_timeliness(self, content: str, metadata: Dict[str, str]) -> List[Dict[str, Any]]:
        """æ™‚æµã¨ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        # å¤ã„å¹´å·ã®å‚ç…§ã‚’ãƒã‚§ãƒƒã‚¯
        current_year = datetime.now().year
        old_year_pattern = r'\b(20[0-9]{2})å¹´'
        year_matches = re.findall(old_year_pattern, content)
        
        for year in year_matches:
            if int(year) < current_year - 2:  # 2å¹´ä»¥ä¸Šå‰
                issues.append({
                    "type": "outdated_reference",
                    "severity": "low",
                    "location": f"{year}å¹´",
                    "original": year,
                    "suggestion": "Consider updating to more recent information",
                    "auto_correctable": False
                })
        
        # ã€Œæœ€æ–°ã€ã¨ã„ã†è¡¨ç¾ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        if "æœ€æ–°" in content:
            # è¨˜äº‹ã®æ—¥ä»˜ã‚’ç¢ºèª
            article_date = metadata.get("date", "")
            if article_date:
                try:
                    # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆYYYY-MM-DD HH:MMå½¢å¼ã‚’æƒ³å®šï¼‰
                    article_datetime = datetime.strptime(article_date, "%Y-%m-%d %H:%M")
                    # ç¾åœ¨æ™‚åˆ»ã¨ã®å·®ãŒ1ãƒ¶æœˆä»¥ä¸Šãªã‚‰è­¦å‘Š
                    if (datetime.now() - article_datetime).days > 30:
                        issues.append({
                            "type": "stale_latest_claim",
                            "severity": "medium",
                            "location": "ã€Œæœ€æ–°ã€ã¨ã„ã†è¡¨ç¾",
                            "original": "æœ€æ–°",
                            "suggestion": "å…·ä½“çš„ãªæ—¥ä»˜ã‚„æ™‚æœŸã‚’æ˜è¨˜",
                            "auto_correctable": False
                        })
                except:
                    pass
        
        return issues
    
    def _check_writing_quality(self, content: str) -> List[Dict[str, Any]]:
        """æ–‡ç« å“è³ªã‚’ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        # åŸºæœ¬çš„ãªèª¤å­—è„±å­—ãƒ‘ã‚¿ãƒ¼ãƒ³
        typo_patterns = [
            ("ã“ã©ã‚‚", "å­ã©ã‚‚"),
            ("ã„ã¥ã‚Œ", "ã„ãšã‚Œ"),
            ("ã™ããªãã¨ã‚‚", "å°‘ãªãã¨ã‚‚"),
            ("ã‚‚ã¨ã¥", "åŸºã¥"),
            ("ãŠã“ãª", "è¡Œãª"),
            ("ã—ã¦ã‚‹", "ã—ã¦ã„ã‚‹"),
            ("ã—ã¦ãªã„", "ã—ã¦ã„ãªã„"),
        ]
        
        for wrong, correct in typo_patterns:
            if wrong in content:
                issues.append({
                    "type": "typo",
                    "severity": "low",
                    "location": wrong,
                    "original": wrong,
                    "suggestion": correct,
                    "auto_correctable": True,
                    "pattern": wrong,
                    "replacement": correct
                })
        
        # ä¸€æ–‡ã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
        for i, sentence in enumerate(sentences):
            if len(sentence) > 100:  # 100æ–‡å­—ä»¥ä¸Š
                issues.append({
                    "type": "long_sentence",
                    "severity": "low",
                    "location": f"Sentence {i+1}",
                    "original": sentence[:50] + "...",
                    "suggestion": "æ–‡ã‚’åˆ†å‰²ã—ã¦èª­ã¿ã‚„ã™ãã™ã‚‹",
                    "auto_correctable": False
                })
        
        # å—å‹•æ…‹ã®éåº¦ãªä½¿ç”¨ã‚’ãƒã‚§ãƒƒã‚¯
        passive_count = len(re.findall(r'ã‚Œã‚‹|ã‚‰ã‚Œã‚‹', content))
        if passive_count > 20:  # è¨˜äº‹å…¨ä½“ã§20å›ä»¥ä¸Š
            issues.append({
                "type": "excessive_passive_voice",
                "severity": "low",
                "location": "å…¨ä½“",
                "original": f"å—å‹•æ…‹ãŒ{passive_count}å›ä½¿ç”¨",
                "suggestion": "èƒ½å‹•æ…‹ã‚’ä½¿ã£ã¦æ–‡ç« ã‚’æ´»ç™ºã«ã™ã‚‹",
                "auto_correctable": False
            })
        
        # å°‚é–€ç”¨èªã®èª¬æ˜ä¸è¶³ã‚’ãƒã‚§ãƒƒã‚¯
        technical_terms = [
            "Docker", "Kubernetes", "CI/CD", "DevOps", "ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹",
            "ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·", "ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ", "å†ªç­‰æ€§", "éåŒæœŸå‡¦ç†"
        ]
        
        content_lower = content.lower()
        for term in technical_terms:
            if term.lower() in content_lower:
                # ç”¨èªã®èª¬æ˜ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                explanation_patterns = [
                    f"{term}ã¨ã¯",
                    f"{term}ã¯",
                    f"{term}ï¼ˆ",
                    f"{term} \\("
                ]
                has_explanation = any(
                    pattern.lower() in content_lower 
                    for pattern in explanation_patterns
                )
                
                if not has_explanation:
                    issues.append({
                        "type": "missing_term_explanation",
                        "severity": "medium",
                        "location": term,
                        "original": term,
                        "suggestion": f"{term}ã®èª¬æ˜ã‚’è¿½åŠ ",
                        "auto_correctable": False
                    })
        
        return issues
    
    def _check_practicality(self, content: str) -> List[Dict[str, Any]]:
        """å®Ÿç”¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        # ã‚³ãƒ¼ãƒ‰ä¾‹ã®æ•°ã‚’ãƒã‚§ãƒƒã‚¯
        code_blocks = re.findall(r'```[^\n]*\n', content)
        if len(code_blocks) < 3:
            issues.append({
                "type": "insufficient_code_examples",
                "severity": "medium",
                "location": "å…¨ä½“",
                "original": f"ã‚³ãƒ¼ãƒ‰ä¾‹ãŒ{len(code_blocks)}å€‹",
                "suggestion": "ã‚ˆã‚Šå¤šãã®å®Ÿè£…ä¾‹ã‚’è¿½åŠ ",
                "auto_correctable": False
            })
        
        # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ã®æœ‰ç„¡ã‚’ãƒã‚§ãƒƒã‚¯
        has_install = any(
            keyword in content 
            for keyword in ["pip install", "npm install", "yarn add", "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"]
        )
        
        if not has_install and any(
            tech in content 
            for tech in ["ãƒ©ã‚¤ãƒ–ãƒ©ãƒª", "ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸", "ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯"]
        ):
            issues.append({
                "type": "missing_installation_guide",
                "severity": "medium",
                "location": "ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ã‚¯ã‚·ãƒ§ãƒ³",
                "original": "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ãªã—",
                "suggestion": "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ã‚’è¿½åŠ ",
                "auto_correctable": False
            })
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®è¨€åŠã‚’ãƒã‚§ãƒƒã‚¯
        has_error_handling = any(
            keyword in content 
            for keyword in ["try", "except", "catch", "ã‚¨ãƒ©ãƒ¼", "ä¾‹å¤–", "ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"]
        )
        
        if not has_error_handling:
            issues.append({
                "type": "missing_error_handling",
                "severity": "low",
                "location": "å…¨ä½“",
                "original": "ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®è¨€åŠãªã—",
                "suggestion": "ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚„ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’è¿½åŠ ",
                "auto_correctable": False
            })
        
        return issues
    
    def _check_code_syntax(self, code: str) -> List[Dict[str, Any]]:
        """ã‚³ãƒ¼ãƒ‰ã®æ–‡æ³•ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        issues = []
        
        # åŸºæœ¬çš„ãªæ–‡æ³•ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
        if "import" in code and "from" in code:
            lines = code.split('\n')
            for i, line in enumerate(lines):
                if line.strip().startswith("from") and " import " in line:
                    # importæ–‡ã®é †åºã‚’ãƒã‚§ãƒƒã‚¯
                    if i > 0 and not lines[i-1].strip().startswith(("from", "import", "")):
                        issues.append({
                            "type": "import_order",
                            "severity": "low",
                            "location": f"Line {i+1}",
                            "original": line.strip(),
                            "suggestion": "Importæ–‡ã‚’å…ˆé ­ã«ã¾ã¨ã‚ã‚‹",
                            "auto_correctable": False
                        })
        
        # æœªä½¿ç”¨ã®å¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        variable_pattern = r'^(\s*)(\w+)\s*='
        variables = re.findall(variable_pattern, code, re.MULTILINE)
        for indent, var_name in variables:
            if var_name not in ["_", "__"] and code.count(var_name) == 1:
                issues.append({
                    "type": "unused_variable",
                    "severity": "low",
                    "location": var_name,
                    "original": var_name,
                    "suggestion": f"æœªä½¿ç”¨ã®å¤‰æ•° {var_name}",
                    "auto_correctable": False
                })
        
        return issues
    
    def _is_version_outdated(self, version: str, latest: str) -> bool:
        """ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒå¤ã„ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            current_parts = [int(x) for x in version.split('.')]
            latest_parts = [int(x) for x in latest.split('.')]
            
            # ãƒ¡ã‚¸ãƒ£ãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§æ¯”è¼ƒ
            if current_parts[0] < latest_parts[0]:
                return True
            
            # ãƒã‚¤ãƒŠãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§æ¯”è¼ƒï¼ˆãƒ¡ã‚¸ãƒ£ãƒ¼ãŒåŒã˜å ´åˆï¼‰
            if current_parts[0] == latest_parts[0] and len(current_parts) > 1 and len(latest_parts) > 1:
                if current_parts[1] < latest_parts[1]:
                    return True
            
            return False
        except:
            return False
    
    async def _is_url_valid(self, url: str) -> bool:
        """URLãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            # GitHubã®URLã¯å¸¸ã«æœ‰åŠ¹ã¨ã¿ãªã™
            parsed = urlparse(url)
            if parsed.netloc in ["github.com", "githubusercontent.com"]:
                return True
            
            # ãã®ä»–ã®URLã¯ç°¡å˜ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯ã®ã¿
            return bool(parsed.scheme and parsed.netloc)
        except:
            return False
    
    def _apply_correction(self, content: str, issue: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:
        """ä¿®æ­£ã‚’é©ç”¨"""
        if not issue.get("auto_correctable", False):
            return content, None
        
        pattern = issue.get("pattern")
        replacement = issue.get("replacement")
        
        if pattern and replacement is not None:
            corrected_content = content.replace(pattern, replacement)
            if corrected_content != content:
                correction = {
                    "type": issue["type"],
                    "original": pattern,
                    "corrected": replacement,
                    "location": issue.get("location", "")
                }
                return corrected_content, correction
        
        return content, None

class ProofreadingRuleManager:
    """æ ¡æ­£ãƒ«ãƒ¼ãƒ«ã‚’ç®¡ç†ãƒ»æ›´æ–°ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.rules_file = Path("BLOG_PROOFREADING_RULES.md")
        self.rules_history_file = Path("proofreading_rules_history.json")
        self.load_rules_history()
    
    def load_rules_history(self):
        """ãƒ«ãƒ¼ãƒ«æ›´æ–°å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€"""
        if self.rules_history_file.exists():
            with open(self.rules_history_file, "r", encoding="utf-8") as f:
                self.rules_history = json.load(f)
        else:
            self.rules_history = {
                "versions": [],
                "update_reasons": []
            }
    
    def save_rules_history(self):
        """ãƒ«ãƒ¼ãƒ«æ›´æ–°å±¥æ­´ã‚’ä¿å­˜"""
        with open(self.rules_history_file, "w", encoding="utf-8") as f:
            json.dump(self.rules_history, f, ensure_ascii=False, indent=2)
    
    async def evaluate_proofreading_rules(self, recent_proofreadings: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æœ€è¿‘ã®æ ¡æ­£çµæœã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒ«ã‚’è©•ä¾¡"""
        
        rule_evaluation = {
            "timestamp": datetime.now(JST).isoformat(),
            "effectiveness": 0,
            "suggested_updates": [],
            "common_issues": {}
        }
        
        if not recent_proofreadings:
            return rule_evaluation
        
        # å…±é€šã®å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        issue_counts = {}
        total_issues = 0
        
        for proofreading in recent_proofreadings:
            for issue in proofreading.get("issues_found", []):
                issue_type = issue["type"]
                issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1
                total_issues += 1
        
        rule_evaluation["common_issues"] = issue_counts
        
        # ãƒ«ãƒ¼ãƒ«ã®æœ‰åŠ¹æ€§ã‚’è©•ä¾¡
        avg_final_score = sum(p["final_score"] for p in recent_proofreadings) / len(recent_proofreadings)
        rule_evaluation["effectiveness"] = avg_final_score
        
        # ãƒ«ãƒ¼ãƒ«æ›´æ–°ã®ææ¡ˆ
        for issue_type, count in issue_counts.items():
            if count >= 3:  # 3å›ä»¥ä¸Šç™ºç”Ÿ
                if issue_type == "version_outdated":
                    rule_evaluation["suggested_updates"].append({
                        "rule": "ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã®è‡ªå‹•æ›´æ–°å¼·åŒ–",
                        "action": "æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã®å–å¾—æ–¹æ³•ã‚’æ”¹å–„",
                        "priority": "high"
                    })
                elif issue_type == "missing_term_explanation":
                    rule_evaluation["suggested_updates"].append({
                        "rule": "å°‚é–€ç”¨èªã®èª¬æ˜ãƒã‚§ãƒƒã‚¯å¼·åŒ–",
                        "action": "ç”¨èªé›†ã‚’ä½œæˆã—ã€è‡ªå‹•çš„ã«èª¬æ˜ã‚’æŒ¿å…¥",
                        "priority": "medium"
                    })
                elif issue_type == "typo":
                    rule_evaluation["suggested_updates"].append({
                        "rule": "èª¤å­—è„±å­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ‹¡å……",
                        "action": "ã‚ˆãã‚ã‚‹èª¤å­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ«ãƒ¼ãƒ«ã«è¿½åŠ ",
                        "priority": "low"
                    })
        
        return rule_evaluation

class ImprovedArticleWithProofreading:
    """æ ¡æ­£æ©Ÿèƒ½ã‚’çµ„ã¿è¾¼ã‚“ã è¨˜äº‹ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.proofreader = ArticleProofreader()
        self.rule_manager = ProofreadingRuleManager()
    
    async def generate_and_proofread(self, article_path: Path) -> Dict[str, Any]:
        """è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦æ ¡æ­£ã™ã‚‹"""
        
        print("ğŸ“ è¨˜äº‹ã®æ ¡æ­£ã‚’é–‹å§‹...")
        
        # è¨˜äº‹ã‚’æ ¡æ­£
        proofreading_result = await self.proofreader.proofread_article(article_path)
        
        print(f"ğŸ“Š æ ¡æ­£çµæœ:")
        print(f"  - å…ƒã®ã‚¹ã‚³ã‚¢: {proofreading_result['original_score']}")
        print(f"  - æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {len(proofreading_result['issues_found'])}ä»¶")
        print(f"  - è‡ªå‹•ä¿®æ­£: {len(proofreading_result['corrections'])}ä»¶")
        print(f"  - æœ€çµ‚ã‚¹ã‚³ã‚¢: {proofreading_result['final_score']}")
        
        # å•é¡Œã®è©³ç´°ã‚’è¡¨ç¤º
        if proofreading_result['issues_found']:
            print("\nâš ï¸  æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ:")
            for issue in proofreading_result['issues_found'][:5]:  # æœ€åˆã®5ä»¶
                print(f"  - [{issue['severity']}] {issue['type']}: {issue['original'][:50]}...")
        
        # è‡ªå‹•ä¿®æ­£ã‚’é©ç”¨
        if proofreading_result.get('auto_corrected', False):
            print("\nâœ… è‡ªå‹•ä¿®æ­£ã‚’é©ç”¨ã—ã¾ã—ãŸ:")
            for correction in proofreading_result['corrections']:
                print(f"  - {correction['type']}: {correction['original']} â†’ {correction['corrected']}")
            
            # ä¿®æ­£ã•ã‚ŒãŸå†…å®¹ã‚’ä¿å­˜
            with open(article_path, 'w', encoding='utf-8') as f:
                f.write(proofreading_result['corrected_content'])
            
            print(f"\nğŸ“ è¨˜äº‹ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {article_path}")
        
        # æ ¡æ­£ãƒ­ã‚°ã‚’ä¿å­˜
        self.proofreader.proofreading_log["logs"].append(proofreading_result)
        self.proofreader.save_proofreading_log()
        
        # ãƒ«ãƒ¼ãƒ«ã®è©•ä¾¡ï¼ˆæœ€æ–°10ä»¶ã®æ ¡æ­£çµæœã‚’ä½¿ç”¨ï¼‰
        recent_logs = self.proofreader.proofreading_log["logs"][-10:]
        rule_evaluation = await self.rule_manager.evaluate_proofreading_rules(recent_logs)
        
        if rule_evaluation["suggested_updates"]:
            print("\nğŸ’¡ æ ¡æ­£ãƒ«ãƒ¼ãƒ«ã®æ”¹å–„ææ¡ˆ:")
            for update in rule_evaluation["suggested_updates"]:
                print(f"  - {update['rule']}: {update['action']}")
        
        return {
            "proofreading_result": proofreading_result,
            "rule_evaluation": rule_evaluation
        }

async def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸ” è¨˜äº‹æ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ  - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 60)
    
    # æœ€æ–°ã®è¨˜äº‹ã‚’å–å¾—
    posts_dir = Path("posts")
    latest_article = sorted(
        posts_dir.glob("*.md"), 
        key=lambda x: x.stat().st_mtime, 
        reverse=True
    )[0]
    
    print(f"\nğŸ“„ å¯¾è±¡è¨˜äº‹: {latest_article.name}")
    
    # æ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè¡Œ
    system = ImprovedArticleWithProofreading()
    result = await system.generate_and_proofread(latest_article)
    
    print("\nâœ… æ ¡æ­£å®Œäº†ï¼")
    
    if result["proofreading_result"]["final_score"] >= 90:
        print("  â†’ é«˜å“è³ªãªè¨˜äº‹ã§ã™ï¼")
    elif result["proofreading_result"]["final_score"] >= 80:
        print("  â†’ è‰¯å¥½ãªå“è³ªã§ã™ãŒã€æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚")
    else:
        print("  â†’ å“è³ªå‘ä¸Šã®ãŸã‚ã€ã•ã‚‰ãªã‚‹ä¿®æ­£ãŒå¿…è¦ã§ã™ã€‚")

if __name__ == "__main__":
    asyncio.run(main())
#!/usr/bin/env python3
"""
RSS Aggregator - å®Ÿéš›ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’åé›†
"""

import feedparser
import httpx
import asyncio
from datetime import datetime, timezone, timedelta
import json
from pathlib import Path
import hashlib

class RSSAggregator:
    def __init__(self):
        self.feeds = [
            {
                "name": "Qiita AI", 
                "url": "https://qiita.com/tags/ai/feed.atom",
                "type": "tech"
            },
            {
                "name": "Qiita Machine Learning",
                "url": "https://qiita.com/tags/machinelearning/feed.atom",
                "type": "tech"
            },
            {
                "name": "Qiita Python",
                "url": "https://qiita.com/tags/python/feed.atom",
                "type": "tech"
            },
            {
                "name": "Dev.to AI",
                "url": "https://dev.to/feed/tag/ai",
                "type": "tech"
            },
            {
                "name": "Reddit r/artificial",
                "url": "https://www.reddit.com/r/artificial/.rss",
                "type": "discussion"
            }
        ]
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.cache_dir = Path("data/rss_cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    async def fetch_feed(self, feed_info):
        """å˜ä¸€ã®ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—"""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(
                    feed_info["url"],
                    headers={
                        'User-Agent': 'Mozilla/5.0 (compatible; AlicAIBot/1.0)'
                    }
                )
                
                if response.status_code == 200:
                    parsed = feedparser.parse(response.text)
                    articles = []
                    
                    for entry in parsed.entries[:10]:  # æœ€æ–°10ä»¶
                        article = {
                            "title": entry.get("title", ""),
                            "link": entry.get("link", ""),
                            "summary": entry.get("summary", "")[:500],  # è¦ç´„ã¯500æ–‡å­—ã¾ã§
                            "source": feed_info["name"],
                            "type": feed_info["type"],
                            "published": entry.get("published", ""),
                            "tags": [tag.term for tag in entry.get("tags", [])][:5],
                            "id": hashlib.md5(entry.get("link", "").encode()).hexdigest()
                        }
                        articles.append(article)
                    
                    return articles
                else:
                    print(f"âŒ Failed to fetch {feed_info['name']}: {response.status_code}")
                    return []
                    
        except Exception as e:
            print(f"âŒ Error fetching {feed_info['name']}: {str(e)}")
            return []
    
    async def fetch_all_feeds(self):
        """ã™ã¹ã¦ã®ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ä¸¦è¡Œã—ã¦å–å¾—"""
        print("ğŸ“¡ RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ä¸­...")
        
        tasks = [self.fetch_feed(feed) for feed in self.feeds]
        results = await asyncio.gather(*tasks)
        
        # çµæœã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–
        all_articles = []
        for articles in results:
            all_articles.extend(articles)
        
        # IDã§é‡è¤‡ã‚’æ’é™¤
        unique_articles = {}
        for article in all_articles:
            unique_articles[article["id"]] = article
        
        articles_list = list(unique_articles.values())
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        cache_file = self.cache_dir / f"articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(articles_list, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {len(articles_list)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
        return articles_list
    
    async def analyze_trends(self, articles):
        """è¨˜äº‹ã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ"""
        # ã‚¿ã‚°ã®å‡ºç¾é »åº¦ã‚’é›†è¨ˆ
        tag_counts = {}
        word_counts = {}
        
        for article in articles:
            # ã‚¿ã‚°é›†è¨ˆ
            for tag in article.get("tags", []):
                tag_lower = tag.lower()
                tag_counts[tag_lower] = tag_counts.get(tag_lower, 0) + 1
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é‡è¦ãã†ãªå˜èªã‚’æŠ½å‡º
            title_words = article["title"].lower().split()
            for word in title_words:
                if len(word) > 3:  # 4æ–‡å­—ä»¥ä¸Šã®å˜èª
                    word_counts[word] = word_counts.get(word, 0) + 1
        
        # ãƒˆãƒƒãƒ—ãƒˆãƒ¬ãƒ³ãƒ‰
        top_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        
        return {
            "top_tags": top_tags,
            "top_words": top_words,
            "total_articles": len(articles),
            "sources": list(set(article["source"] for article in articles))
        }
    
    async def generate_curated_article(self, articles):
        """åé›†ã—ãŸè¨˜äº‹ã‹ã‚‰ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨˜äº‹ã‚’ç”Ÿæˆ"""
        jst_now = datetime.now(timezone(timedelta(hours=9)))
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        trends = await self.analyze_trends(articles)
        
        # è¨˜äº‹ID
        article_id = f"curated_{int(jst_now.timestamp())}"
        
        # è¨˜äº‹å†…å®¹ã‚’ç”Ÿæˆ
        content = f"""# ğŸ“¡ AIãƒ†ãƒƒã‚¯æ¥­ç•Œã®æœ€æ–°å‹•å‘ - ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

*{jst_now.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')} JST*

AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä¸–ç•Œä¸­ã®ãƒ†ãƒƒã‚¯ãƒ–ãƒ­ã‚°ã‹ã‚‰åé›†ã—ãŸæœ€æ–°æƒ…å ±ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚

## ğŸ“Š ä»Šæ—¥ã®ãƒˆãƒ¬ãƒ³ãƒ‰

### ğŸ·ï¸ æ³¨ç›®ã®ã‚¿ã‚°
{chr(10).join([f"- **{tag}** ({count}ä»¶)" for tag, count in trends['top_tags'][:5]])}

### ğŸ“ˆ é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
{', '.join([f"`{word}`" for word, _ in trends['top_words'][:10]])}

## ğŸŒŸ æ³¨ç›®è¨˜äº‹ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—

"""
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«è¨˜äº‹ã‚’æ•´ç†
        tech_articles = [a for a in articles if a["type"] == "tech"][:5]
        discussion_articles = [a for a in articles if a["type"] == "discussion"][:3]
        
        if tech_articles:
            content += "### ğŸ”§ æŠ€è¡“è¨˜äº‹\n\n"
            for article in tech_articles:
                tags_str = ", ".join(article["tags"][:3]) if article["tags"] else "AI"
                content += f"""**[{article['title']}]({article['link']})**
- ğŸ·ï¸ {tags_str}
- ğŸ“° {article['source']}
- ğŸ“ {article['summary'][:200]}...

"""
        
        if discussion_articles:
            content += "### ğŸ’¬ ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³\n\n"
            for article in discussion_articles:
                content += f"""**[{article['title']}]({article['link']})**
- ğŸ“° {article['source']}
- ğŸ’­ {article['summary'][:150]}...

"""
        
        content += f"""## ğŸ“ˆ çµ±è¨ˆæƒ…å ±

- åé›†è¨˜äº‹æ•°: {trends['total_articles']}ä»¶
- æƒ…å ±æº: {', '.join(trends['sources'])}
- åé›†æ™‚åˆ»: {jst_now.strftime('%Y-%m-%d %H:%M:%S')} JST

---

*ã“ã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨˜äº‹ã¯ã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè‡ªå‹•çš„ã«åé›†ãƒ»åˆ†æãƒ»ç”Ÿæˆã—ã¾ã—ãŸã€‚*
*æ¬¡å›æ›´æ–°äºˆå®š: {(jst_now + timedelta(hours=4)).strftime('%Y-%m-%d %H:%M')} JST*"""
        
        # è¨˜äº‹ã‚’ä¿å­˜
        posts_dir = Path("posts")
        article_path = posts_dir / f"{article_id}.md"
        
        with open(article_path, "w", encoding="utf-8") as f:
            f.write(f"---\n")
            f.write(f"title: ğŸ“¡ AIãƒ†ãƒƒã‚¯æ¥­ç•Œã®æœ€æ–°å‹•å‘ - {jst_now.strftime('%m/%d')}\n")
            f.write(f"date: {jst_now.strftime('%Y-%m-%d %H:%M')}\n")
            f.write(f"tags: Curation, AI, Trends\n")
            f.write(f"source: RSS Aggregation\n")
            f.write(f"---\n\n")
            f.write(content)
        
        print(f"âœ… ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨˜äº‹ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {article_id}")
        return article_id

async def test_rss_aggregator():
    """RSSã‚¢ã‚°ãƒªã‚²ãƒ¼ã‚¿ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    aggregator = RSSAggregator()
    
    # ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
    articles = await aggregator.fetch_all_feeds()
    
    if articles:
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        trends = await aggregator.analyze_trends(articles)
        print(f"\nğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ:")
        print(f"   ãƒˆãƒƒãƒ—ã‚¿ã‚°: {[tag for tag, _ in trends['top_tags'][:5]]}")
        print(f"   è¨˜äº‹æ•°: {trends['total_articles']}")
        
        # ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨˜äº‹ã‚’ç”Ÿæˆ
        await aggregator.generate_curated_article(articles)
        
        # HTMLå¤‰æ›
        import subprocess
        subprocess.run(["python", "convert_articles.py"], capture_output=True)
        subprocess.run(["python", "update_to_modern_ui.py"], capture_output=True)
        
        print("\nâœ¨ RSSå®Ÿè£…ã®ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")

if __name__ == "__main__":
    asyncio.run(test_rss_aggregator())
#!/usr/bin/env python3
"""
è‡ªå·±é€²åŒ–ãƒ—ãƒ­ã‚»ã‚¹è¨˜éŒ²ã‚·ã‚¹ãƒ†ãƒ 
ã‚·ã‚¹ãƒ†ãƒ ã®æ”¹å–„éç¨‹ã‚’è¿½è·¡ãƒ»åˆ†æã—ã€é€²åŒ–ã®è»Œè·¡ã‚’è¨˜éŒ²ã™ã‚‹
"""

import json
import os
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import statistics

# æ—¥æœ¬æ¨™æº–æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³
JST = timezone(timedelta(hours=9))

def get_jst_now():
    """ç¾åœ¨ã®æ—¥æœ¬æ™‚é–“ã‚’å–å¾—"""
    return datetime.now(JST)

class SelfEvolutionTracker:
    """è‡ªå·±é€²åŒ–ãƒ—ãƒ­ã‚»ã‚¹ãƒˆãƒ©ãƒƒã‚«ãƒ¼"""
    
    def __init__(self, evolution_file="evolution_history.json"):
        self.evolution_file = evolution_file
        self.evolution_data = self._load_evolution_data()
        
    def _load_evolution_data(self):
        """é€²åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.evolution_file):
            try:
                with open(self.evolution_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"Error loading evolution data: {e}")
        
        return {
            "evolution_sessions": [],
            "improvement_patterns": {},
            "metrics_over_time": {},
            "system_milestones": []
        }
    
    def _save_evolution_data(self):
        """é€²åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            with open(self.evolution_file, 'w', encoding='utf-8') as f:
                json.dump(self.evolution_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Error saving evolution data: {e}")
    
    def analyze_current_state(self):
        """ç¾åœ¨ã®çŠ¶æ…‹ã‚’åˆ†æ"""
        print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ ç¾çŠ¶åˆ†æ")
        print("=" * 60)
        
        # è©•ä¾¡å±¥æ­´ã®åˆ†æ
        evaluation_analysis = self._analyze_evaluations()
        
        # ãƒ«ãƒ¼ãƒ«å¤‰æ›´å±¥æ­´ã®åˆ†æ
        rules_analysis = self._analyze_rules_changes()
        
        # è¨˜äº‹å“è³ªã®æ¨ç§»åˆ†æ
        quality_trends = self._analyze_quality_trends()
        
        # æŠ€è¡“çš„æ”¹å–„ã®åˆ†æ
        technical_improvements = self._analyze_technical_improvements()
        
        analysis_result = {
            "timestamp": get_jst_now().isoformat(),
            "evaluation_analysis": evaluation_analysis,
            "rules_analysis": rules_analysis,
            "quality_trends": quality_trends,
            "technical_improvements": technical_improvements,
            "overall_score": self._calculate_overall_score(evaluation_analysis, quality_trends)
        }
        
        return analysis_result
    
    def _analyze_evaluations(self):
        """è©•ä¾¡å±¥æ­´ã‚’åˆ†æ"""
        try:
            with open("evaluation_history.json", 'r', encoding='utf-8') as f:
                eval_data = json.load(f)
            
            evaluations = eval_data.get("evaluations", [])
            if not evaluations:
                return {"error": "No evaluation data found"}
            
            # ã‚¹ã‚³ã‚¢ã®æ¨ç§»ã‚’åˆ†æ
            scores_by_category = {
                "technical_accuracy": [],
                "readability": [],
                "practicality": [],
                "originality": [],
                "total_score": []
            }
            
            articles_by_date = {}
            
            for eval in evaluations:
                scores = eval.get("scores", {})
                date = eval.get("timestamp", "")[:10]  # YYYY-MM-DD
                
                if date not in articles_by_date:
                    articles_by_date[date] = []
                
                articles_by_date[date].append({
                    "title": eval.get("metadata", {}).get("title", ""),
                    "total_score": eval.get("total_score", 0),
                    "scores": scores
                })
                
                for category, score in scores.items():
                    if category in scores_by_category:
                        scores_by_category[category].append(score)
                
                if "total_score" in eval:
                    scores_by_category["total_score"].append(eval["total_score"])
            
            # çµ±è¨ˆè¨ˆç®—
            statistics_data = {}
            for category, scores in scores_by_category.items():
                if scores:
                    statistics_data[category] = {
                        "average": round(statistics.mean(scores), 2),
                        "median": round(statistics.median(scores), 2),
                        "max": max(scores),
                        "min": min(scores),
                        "count": len(scores),
                        "trend": self._calculate_trend(scores)
                    }
            
            # æœ€è¿‘ã®æ”¹å–„ç‚¹ã‚’ç‰¹å®š
            recent_issues = self._identify_recent_issues(evaluations)
            
            return {
                "total_evaluations": len(evaluations),
                "statistics": statistics_data,
                "articles_by_date": articles_by_date,
                "recent_issues": recent_issues,
                "improvement_areas": self._identify_improvement_areas(statistics_data)
            }
            
        except FileNotFoundError:
            return {"error": "evaluation_history.json not found"}
        except Exception as e:
            return {"error": str(e)}
    
    def _analyze_rules_changes(self):
        """ãƒ«ãƒ¼ãƒ«å¤‰æ›´å±¥æ­´ã‚’åˆ†æ"""
        try:
            with open("rules_history.json", 'r', encoding='utf-8') as f:
                rules_data = json.load(f)
            
            versions = rules_data.get("versions", [])
            
            rule_evolution = []
            for version in versions:
                rule_evolution.append({
                    "version": version.get("version"),
                    "date": version.get("date"),
                    "updates": version.get("updates", []),
                    "rationale": version.get("rationale", [])
                })
            
            return {
                "total_rule_updates": len(versions),
                "rule_evolution": rule_evolution,
                "most_common_issues": self._analyze_common_rule_patterns(versions)
            }
            
        except FileNotFoundError:
            return {"error": "rules_history.json not found"}
        except Exception as e:
            return {"error": str(e)}
    
    def _analyze_quality_trends(self):
        """å“è³ªã®æ¨ç§»ã‚’åˆ†æ"""
        # éå»ã®ãƒ–ãƒ­ã‚°æŠ•ç¨¿ã‹ã‚‰å“è³ªæŒ‡æ¨™ã‚’æŠ½å‡º
        posts_dir = Path("posts")
        if not posts_dir.exists():
            return {"error": "Posts directory not found"}
        
        quality_metrics = []
        
        for md_file in posts_dir.glob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # è¨˜äº‹ã®å“è³ªæŒ‡æ¨™ã‚’è¨ˆç®—
                metrics = self._calculate_article_quality_metrics(content, md_file.name)
                if metrics:
                    quality_metrics.append(metrics)
                    
            except Exception as e:
                print(f"Error analyzing {md_file}: {e}")
        
        # æ™‚ç³»åˆ—ã§å“è³ªã®å¤‰åŒ–ã‚’åˆ†æ
        quality_metrics.sort(key=lambda x: x['timestamp'])
        
        return {
            "total_articles": len(quality_metrics),
            "quality_progression": quality_metrics[-10:],  # æœ€æ–°10è¨˜äº‹
            "quality_improvements": self._identify_quality_improvements(quality_metrics)
        }
    
    def _analyze_technical_improvements(self):
        """æŠ€è¡“çš„æ”¹å–„ã‚’åˆ†æ"""
        improvements = [
            {
                "improvement": "è©³ç´°è¨˜äº‹ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ  v4.0",
                "date": "2025-06-28",
                "description": "10,000æ–‡å­—ä»¥ä¸Šã®è©³ç´°ãªè¨˜äº‹ç”Ÿæˆæ©Ÿèƒ½ã‚’å®Ÿè£…",
                "impact": "è¨˜äº‹ã®æ–‡å­—æ•°ãŒ1,864æ–‡å­—ã‹ã‚‰37,244æ–‡å­—ã«å¤§å¹…å¢—åŠ ",
                "metrics": {
                    "character_increase": "1,900%",
                    "content_depth": "å¤§å¹…å‘ä¸Š",
                    "user_feedback_addressed": ["è¨˜äº‹ãŒçŸ­ã„", "ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒæ„å‘³ãŒãªã„", "ã‚¼ãƒ­ãƒˆãƒ©ã‚¹ãƒˆ/BeyondCorpã®èª¬æ˜ä¸è¶³"]
                }
            },
            {
                "improvement": "ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½",
                "date": "2025-06-28",
                "description": "å¤ã„è¨˜äº‹ã‚’å‰Šé™¤ã›ãšãƒšãƒ¼ã‚¸åˆ†ã‘ã—ã¦è¡¨ç¤ºã™ã‚‹æ©Ÿèƒ½",
                "impact": "å…¨è¨˜äº‹ãŒé–²è¦§å¯èƒ½ã«ãªã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ãŒå‘ä¸Š",
                "metrics": {
                    "articles_preserved": "100%",
                    "navigation_improvement": "ãƒªãƒ³ã‚¯ãƒ™ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…"
                }
            },
            {
                "improvement": "åˆ¶ä½œæ™‚é–“è¨˜éŒ²ãƒ»è¡¨ç¤ºæ©Ÿèƒ½",
                "date": "2025-06-28", 
                "description": "è¨˜äº‹ã®åˆ¶ä½œæ™‚é–“ã‚’è¨˜éŒ²ã—ã€è¨˜äº‹å†…ã«è¡¨ç¤ºã™ã‚‹æ©Ÿèƒ½",
                "impact": "é€æ˜æ€§ã¨AIãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å¯è¦–åŒ–",
                "metrics": {
                    "transparency_increase": "åˆ¶ä½œæ™‚é–“è¡¨ç¤º",
                    "phase_tracking": "è©³ç´°ãªãƒ•ã‚§ãƒ¼ã‚ºãƒ­ã‚°"
                }
            },
            {
                "improvement": "å…ƒãƒã‚¿è¨˜äº‹ç´¹ä»‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³",
                "date": "2025-06-28",
                "description": "å‚è€ƒã«ã—ãŸè¨˜äº‹ã®è©³ç´°ãªç´¹ä»‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ",
                "impact": "è¨˜äº‹ã®ä¿¡é ¼æ€§ã¨å‚è€ƒè³‡æ–™ã®æ˜ç¢ºåŒ–",
                "metrics": {
                    "reference_quality": "ã‚¿ã‚¤ãƒˆãƒ«ä»˜ããƒªãƒ³ã‚¯ã¨è©³ç´°èª¬æ˜",
                    "credibility_boost": "æƒ…å ±æºã®é€æ˜æ€§å‘ä¸Š"
                }
            }
        ]
        
        return {
            "implemented_improvements": improvements,
            "improvement_velocity": len(improvements),
            "impact_assessment": "é«˜ã„æ”¹å–„åŠ¹æœã‚’å®Ÿç¾"
        }
    
    def _calculate_trend(self, scores):
        """ã‚¹ã‚³ã‚¢ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¨ˆç®—"""
        if len(scores) < 2:
            return "insufficient_data"
        
        # æœ€æ–°ã®å€¤ã¨éå»ã®å¹³å‡ã‚’æ¯”è¼ƒ
        recent = scores[-3:] if len(scores) >= 3 else scores[-1:]
        older = scores[:-3] if len(scores) >= 6 else scores[:-1] if len(scores) > 1 else []
        
        if not older:
            return "insufficient_data"
        
        recent_avg = statistics.mean(recent)
        older_avg = statistics.mean(older)
        
        if recent_avg > older_avg + 2:
            return "improving"
        elif recent_avg < older_avg - 2:
            return "declining"
        else:
            return "stable"
    
    def _identify_recent_issues(self, evaluations):
        """æœ€è¿‘ã®èª²é¡Œã‚’ç‰¹å®š"""
        recent_evaluations = evaluations[-5:]  # æœ€æ–°5ä»¶
        issues = {}
        
        for eval in recent_evaluations:
            weaknesses = eval.get("weaknesses", [])
            for weakness in weaknesses:
                if weakness not in issues:
                    issues[weakness] = 0
                issues[weakness] += 1
        
        # é »åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_issues = sorted(issues.items(), key=lambda x: x[1], reverse=True)
        return [{"issue": issue, "frequency": freq} for issue, freq in sorted_issues[:5]]
    
    def _identify_improvement_areas(self, statistics_data):
        """æ”¹å–„é ˜åŸŸã‚’ç‰¹å®š"""
        areas = []
        
        for category, stats in statistics_data.items():
            if category == "total_score":
                continue
                
            if stats["average"] < 18:  # 25ç‚¹æº€ç‚¹ã®72%æœªæº€
                areas.append({
                    "category": category,
                    "current_average": stats["average"],
                    "priority": "high" if stats["average"] < 15 else "medium",
                    "trend": stats["trend"]
                })
        
        return areas
    
    def _analyze_common_rule_patterns(self, versions):
        """å…±é€šã®ãƒ«ãƒ¼ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        issue_patterns = {}
        
        for version in versions:
            for update in version.get("updates", []):
                rule = update.get("rule", "")
                if "ä¸è¶³" in rule:
                    issue_type = "content_insufficiency"
                elif "æ”¹å–„ãŒå¿…è¦" in rule:
                    issue_type = "quality_improvement"
                else:
                    issue_type = "general_enhancement"
                
                if issue_type not in issue_patterns:
                    issue_patterns[issue_type] = 0
                issue_patterns[issue_type] += 1
        
        return issue_patterns
    
    def _calculate_article_quality_metrics(self, content, filename):
        """è¨˜äº‹ã®å“è³ªæŒ‡æ¨™ã‚’è¨ˆç®—"""
        try:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            metadata = {}
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    metadata_text = parts[1]
                    content_body = parts[2]
                    
                    for line in metadata_text.strip().split('\n'):
                        if ':' in line:
                            key, value = line.split(':', 1)
                            metadata[key.strip()] = value.strip()
            else:
                content_body = content
            
            # å“è³ªæŒ‡æ¨™è¨ˆç®—
            character_count = len(content_body)
            code_blocks = content_body.count('```')
            sections = content_body.count('##')
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡º
            timestamp = self._extract_timestamp_from_filename(filename)
            
            return {
                "filename": filename,
                "timestamp": timestamp,
                "character_count": character_count,
                "code_blocks": code_blocks,
                "sections": sections,
                "has_thought_process": "æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹" in content_body,
                "has_references": "å‚è€ƒ" in content_body or "ãƒªãƒ³ã‚¯" in content_body,
                "production_time": metadata.get("production_time", "ä¸æ˜"),
                "reading_time": metadata.get("reading_time", "ä¸æ˜"),
                "difficulty": metadata.get("difficulty", "ä¸æ˜")
            }
            
        except Exception as e:
            print(f"Error calculating metrics for {filename}: {e}")
            return None
    
    def _extract_timestamp_from_filename(self, filename):
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡º"""
        import re
        match = re.search(r'article_(\d+)', filename)
        if match:
            return int(match.group(1))
        return 0
    
    def _identify_quality_improvements(self, quality_metrics):
        """å“è³ªæ”¹å–„ã‚’ç‰¹å®š"""
        if len(quality_metrics) < 2:
            return []
        
        improvements = []
        
        # æ–‡å­—æ•°ã®æ”¹å–„
        recent_chars = [m['character_count'] for m in quality_metrics[-3:]]
        older_chars = [m['character_count'] for m in quality_metrics[:-3]] if len(quality_metrics) > 3 else []
        
        if older_chars and recent_chars:
            recent_avg = statistics.mean(recent_chars)
            older_avg = statistics.mean(older_chars)
            
            if recent_avg > older_avg * 1.5:  # 50%ä»¥ä¸Šã®å¢—åŠ 
                improvements.append({
                    "metric": "character_count",
                    "improvement": f"{recent_avg:.0f}æ–‡å­— (å‰æœŸæ¯” {((recent_avg/older_avg - 1) * 100):.1f}%å‘ä¸Š)",
                    "significance": "major"
                })
        
        # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®è¿½åŠ 
        recent_with_thought = sum(1 for m in quality_metrics[-5:] if m.get('has_thought_process', False))
        if recent_with_thought >= 3:
            improvements.append({
                "metric": "thought_process_inclusion",
                "improvement": f"æœ€æ–°5è¨˜äº‹ä¸­{recent_with_thought}è¨˜äº‹ã«æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¿½åŠ ",
                "significance": "moderate"
            })
        
        return improvements
    
    def _calculate_overall_score(self, evaluation_analysis, quality_trends):
        """ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        try:
            # è©•ä¾¡ã‚¹ã‚³ã‚¢ã®å¹³å‡
            eval_score = 0
            if "statistics" in evaluation_analysis and "total_score" in evaluation_analysis["statistics"]:
                eval_score = evaluation_analysis["statistics"]["total_score"]["average"]
            
            # å“è³ªæ”¹å–„ã®è©•ä¾¡
            quality_score = 50  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
            if "quality_improvements" in quality_trends:
                quality_score += len(quality_trends["quality_improvements"]) * 10
            
            # ç·åˆã‚¹ã‚³ã‚¢ (0-100)
            overall = min(100, (eval_score + quality_score) / 2)
            
            return {
                "overall_score": round(overall, 1),
                "evaluation_component": round(eval_score, 1),
                "quality_component": round(quality_score, 1),
                "grade": self._get_grade(overall)
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    def _get_grade(self, score):
        """ã‚¹ã‚³ã‚¢ã‹ã‚‰ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’ç®—å‡º"""
        if score >= 90:
            return "A+"
        elif score >= 80:
            return "A"
        elif score >= 70:
            return "B+"
        elif score >= 60:
            return "B"
        else:
            return "C"
    
    def record_evolution_session(self, session_data):
        """é€²åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²"""
        session = {
            "session_id": f"evolution_{int(get_jst_now().timestamp())}",
            "timestamp": get_jst_now().isoformat(),
            "analysis": session_data,
            "improvements_implemented": [],
            "next_steps": []
        }
        
        self.evolution_data["evolution_sessions"].append(session)
        self._save_evolution_data()
        
        print(f"âœ… é€²åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²å®Œäº†: {session['session_id']}")
        
        return session["session_id"]
    
    def generate_evolution_report(self):
        """é€²åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        analysis = self.analyze_current_state()
        session_id = self.record_evolution_session(analysis)
        
        report = f"""
# ğŸ§¬ Alic AI Blog è‡ªå·±é€²åŒ–ãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {get_jst_now().strftime('%Y-%m-%d %H:%M:%S JST')}
**ã‚»ãƒƒã‚·ãƒ§ãƒ³ID**: {session_id}

## ğŸ“Š ç¾çŠ¶åˆ†æ

### è¨˜äº‹è©•ä¾¡çµ±è¨ˆ
"""
        
        if "statistics" in analysis["evaluation_analysis"]:
            stats = analysis["evaluation_analysis"]["statistics"]
            for category, data in stats.items():
                report += f"- **{category}**: å¹³å‡ {data['average']}/25 (ãƒˆãƒ¬ãƒ³ãƒ‰: {data['trend']})\n"
        
        report += f"""
### ç·åˆã‚¹ã‚³ã‚¢
- **ç·åˆè©•ä¾¡**: {analysis['overall_score']['overall_score']}/100 (ã‚°ãƒ¬ãƒ¼ãƒ‰: {analysis['overall_score']['grade']})
- **è©•ä¾¡ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**: {analysis['overall_score']['evaluation_component']}/100
- **å“è³ªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**: {analysis['overall_score']['quality_component']}/100

## ğŸš€ å®Ÿè£…æ¸ˆã¿æ”¹å–„

"""
        
        for improvement in analysis["technical_improvements"]["implemented_improvements"]:
            report += f"""### {improvement['improvement']}
- **å®Ÿè£…æ—¥**: {improvement['date']}
- **èª¬æ˜**: {improvement['description']}
- **åŠ¹æœ**: {improvement['impact']}

"""
        
        report += """## ğŸ“ˆ å“è³ªã®é€²åŒ–

"""
        
        if "quality_improvements" in analysis["quality_trends"]:
            for improvement in analysis["quality_trends"]["quality_improvements"]:
                report += f"- **{improvement['metric']}**: {improvement['improvement']} ({improvement['significance']})\n"
        
        report += """
## ğŸ¯ ä»Šå¾Œã®æ”¹å–„è¨ˆç”»

### é«˜å„ªå…ˆåº¦
"""
        
        if "improvement_areas" in analysis["evaluation_analysis"]:
            high_priority = [area for area in analysis["evaluation_analysis"]["improvement_areas"] if area['priority'] == 'high']
            for area in high_priority:
                report += f"- **{area['category']}**: ç¾åœ¨å¹³å‡ {area['current_average']}/25 (ãƒˆãƒ¬ãƒ³ãƒ‰: {area['trend']})\n"
        
        report += """
### æœ€è¿‘ã®èª²é¡Œ
"""
        
        if "recent_issues" in analysis["evaluation_analysis"]:
            for issue in analysis["evaluation_analysis"]["recent_issues"][:3]:
                report += f"- {issue['issue']} (é »åº¦: {issue['frequency']}å›)\n"
        
        report += """
## ğŸ”„ ç¶™ç¶šçš„æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«

1. **å•é¡Œæ¤œå‡º**: è©•ä¾¡å±¥æ­´ã¨å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è‡ªå‹•åˆ†æ
2. **æ”¹å–„è¨ˆç”»**: ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®æ”¹å–„æˆ¦ç•¥ç«‹æ¡ˆ
3. **å®Ÿè£…å®Ÿè¡Œ**: å…·ä½“çš„ãªæ©Ÿèƒ½æ”¹å–„ã¨ãƒ«ãƒ¼ãƒ«æ›´æ–°
4. **åŠ¹æœæ¸¬å®š**: æ”¹å–„åŠ¹æœã®å®šé‡çš„è©•ä¾¡
5. **å­¦ç¿’çµ±åˆ**: çŸ¥è¦‹ã®ã‚·ã‚¹ãƒ†ãƒ çµ„ã¿è¾¼ã¿

---

*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå·±æ”¹å–„å‹AIã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*
"""
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        report_filename = f"evolution_report_{session_id}.md"
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“‹ é€²åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_filename}")
        
        return report, report_filename

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ§¬ è‡ªå·±é€²åŒ–ãƒ—ãƒ­ã‚»ã‚¹è¨˜éŒ²ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    
    tracker = SelfEvolutionTracker()
    
    # é€²åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    report, filename = tracker.generate_evolution_report()
    
    print(f"\nğŸ“‹ é€²åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç¢ºèª: {filename}")
    print("\n" + "="*60)
    print("ã‚·ã‚¹ãƒ†ãƒ ã®è‡ªå·±é€²åŒ–ãƒ—ãƒ­ã‚»ã‚¹ãŒæ­£å¸¸ã«è¨˜éŒ²ã•ã‚Œã¾ã—ãŸï¼")

if __name__ == "__main__":
    main()
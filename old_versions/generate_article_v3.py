#!/usr/bin/env python3
"""
Enhanced GitHub Actionsè¨˜äº‹ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ v3
- AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæŠ˜ã‚ŠãŸãŸã¿å¯èƒ½ï¼‰
- ã‚ˆã‚Šä¸å¯§ã§è©³ç´°ãªè§£èª¬
- æ”¹å–„ã•ã‚ŒãŸãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã¨ãƒãƒ¼ã‚¸ãƒ³
- å‚è€ƒå…ƒãƒªãƒ³ã‚¯ã®ä½ç½®æ”¹å–„
"""

import asyncio
from datetime import datetime, timezone, timedelta
from pathlib import Path
import json
import time
import os
import random
import re

# æ—¥æœ¬æ¨™æº–æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³
JST = timezone(timedelta(hours=9))

def get_jst_now():
    """ç¾åœ¨ã®æ—¥æœ¬æ™‚é–“ã‚’å–å¾—"""
    return datetime.now(JST)

# ã‚«ãƒ†ã‚´ãƒªãƒ¼ã¨ã‚¿ã‚°ã®å®šç¾©
CATEGORIES = {
    "ai_development": {
        "name": "AIé–‹ç™º",
        "tags": ["AI", "æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "é–‹ç™º"],
        "color": "#667eea"
    },
    "web_tech": {
        "name": "WebæŠ€è¡“",
        "tags": ["Web", "ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰", "ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", "API"],
        "color": "#48bb78"
    },
    "infrastructure": {
        "name": "ã‚¤ãƒ³ãƒ•ãƒ©",
        "tags": ["ã‚¯ãƒ©ã‚¦ãƒ‰", "DevOps", "ã‚¤ãƒ³ãƒ•ãƒ©", "è‡ªå‹•åŒ–"],
        "color": "#ed8936"
    },
    "security": {
        "name": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
        "tags": ["ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼", "èªè¨¼", "æš—å·åŒ–"],
        "color": "#e53e3e"
    },
    "data_science": {
        "name": "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹",
        "tags": ["ãƒ‡ãƒ¼ã‚¿åˆ†æ", "ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿", "çµ±è¨ˆ", "å¯è¦–åŒ–"],
        "color": "#38b2ac"
    }
}

# æ‹¡å¼µã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆã‚ˆã‚Šé­…åŠ›çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã¨è©³ç´°ãªæƒ…å ±ï¼‰
TOPICS = [
    {
        "title": "ã€2025å¹´æœ€æ–°ã€‘AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå¤‰ãˆã‚‹é–‹ç™ºç¾å ´ - AutoGenã¨LangChainã®å®Ÿè·µæ¯”è¼ƒ",
        "short_title": "AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ€æ–°å‹•å‘",
        "category": "ai_development",
        "source_url": "https://github.com/microsoft/autogen",
        "reference_sites": [
            "https://qiita.com/",
            "https://zenn.dev/",
            "https://b.hatena.ne.jp/hotentry/it"
        ],
        "keywords": ["AutoGen", "LangChain", "ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ", "è‡ªå¾‹å‹AI"],
        "difficulty": "ä¸­ç´š",
        "reading_time": "15åˆ†"
    },
    {
        "title": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰:ChatGPT/Claude/Geminiã‚’æœ€å¤§é™æ´»ç”¨ã™ã‚‹15ã®å®Ÿè·µãƒ†ã‚¯ãƒ‹ãƒƒã‚¯",
        "short_title": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å®Ÿè·µãƒ†ã‚¯ãƒ‹ãƒƒã‚¯",
        "category": "ai_development",
        "source_url": "https://github.com/dair-ai/Prompt-Engineering-Guide",
        "reference_sites": [
            "https://qiita.com/tags/chatgpt",
            "https://zenn.dev/topics/prompt",
            "https://dev.to/t/ai"
        ],
        "keywords": ["ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", "ChatGPT", "Claude", "Gemini", "LLM"],
        "difficulty": "åˆç´š",
        "reading_time": "20åˆ†"
    },
    {
        "title": "ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIå®Ÿè£…å…¥é–€:ç”»åƒÃ—ãƒ†ã‚­ã‚¹ãƒˆÃ—éŸ³å£°ã‚’çµ±åˆã™ã‚‹æœ€æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£",
        "short_title": "ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®å¿œç”¨äº‹ä¾‹",
        "category": "ai_development",
        "source_url": "https://github.com/openai/CLIP",
        "reference_sites": [
            "https://huggingface.co/models",
            "https://paperswithcode.com/",
            "https://arxiv.org/"
        ],
        "keywords": ["CLIP", "Vision Transformer", "ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«", "ç”»åƒèªè­˜"],
        "difficulty": "ä¸Šç´š",
        "reading_time": "25åˆ†"
    },
    {
        "title": "RAGã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰ã®æ±ºå®šç‰ˆ:Retrieval-Augmented Generationã§ä½œã‚‹çŸ¥è­˜ãƒ™ãƒ¼ã‚¹AI",
        "short_title": "RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã‚¬ã‚¤ãƒ‰",
        "category": "ai_development",
        "source_url": "https://github.com/langchain-ai/langchain",
        "reference_sites": [
            "https://qiita.com/tags/rag",
            "https://zenn.dev/topics/langchain",
            "https://medium.com/tag/rag"
        ],
        "keywords": ["RAG", "ãƒ™ã‚¯ãƒˆãƒ«DB", "Embeddings", "æ¤œç´¢æ‹¡å¼µç”Ÿæˆ"],
        "difficulty": "ä¸­ç´š",
        "reading_time": "30åˆ†"
    },
    {
        "title": "Next.js 15å®Œå…¨æ”»ç•¥:App RouterÃ—Server ComponentsÃ—Streamingã§ä½œã‚‹çˆ†é€ŸWebã‚¢ãƒ—ãƒª",
        "short_title": "Next.js 15ã®æ–°æ©Ÿèƒ½è§£èª¬",
        "category": "web_tech",
        "source_url": "https://github.com/vercel/next.js",
        "reference_sites": [
            "https://nextjs.org/blog",
            "https://vercel.com/blog",
            "https://dev.to/t/nextjs"
        ],
        "keywords": ["Next.js", "React", "Server Components", "App Router"],
        "difficulty": "ä¸­ç´š",
        "reading_time": "18åˆ†"
    },
    {
        "title": "KubernetesÃ—GitOpså®Ÿè·µ:ArgoCDã¨Fluxã§å®Ÿç¾ã™ã‚‹å®Œå…¨è‡ªå‹•åŒ–ã‚¤ãƒ³ãƒ•ãƒ©",
        "short_title": "K8s GitOpså®Ÿè·µã‚¬ã‚¤ãƒ‰",
        "category": "infrastructure",
        "source_url": "https://github.com/argoproj/argo-cd",
        "reference_sites": [
            "https://kubernetes.io/blog/",
            "https://www.cncf.io/blog/",
            "https://itnext.io/"
        ],
        "keywords": ["Kubernetes", "GitOps", "ArgoCD", "Flux", "CI/CD"],
        "difficulty": "ä¸Šç´š",
        "reading_time": "35åˆ†"
    },
    {
        "title": "ã‚¼ãƒ­ãƒˆãƒ©ã‚¹ãƒˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…ã‚¬ã‚¤ãƒ‰:BeyondCorpãƒ¢ãƒ‡ãƒ«ã§ä½œã‚‹æ¬¡ä¸–ä»£èªè¨¼åŸºç›¤",
        "short_title": "ã‚¼ãƒ­ãƒˆãƒ©ã‚¹ãƒˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å…¥é–€",
        "category": "security",
        "source_url": "https://github.com/pomerium/pomerium",
        "reference_sites": [
            "https://www.csoonline.com/",
            "https://www.darkreading.com/",
            "https://thehackernews.com/"
        ],
        "keywords": ["ã‚¼ãƒ­ãƒˆãƒ©ã‚¹ãƒˆ", "BeyondCorp", "èªè¨¼", "ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡"],
        "difficulty": "ä¸Šç´š",
        "reading_time": "40åˆ†"
    },
    {
        "title": "å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æ–°å¸¸è­˜:Apache Spark vs Databricks vs Snowflakeå¾¹åº•æ¯”è¼ƒ",
        "short_title": "ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŸºç›¤ã®é¸ã³æ–¹",
        "category": "data_science",
        "source_url": "https://github.com/apache/spark",
        "reference_sites": [
            "https://databricks.com/blog",
            "https://www.snowflake.com/blog/",
            "https://towardsdatascience.com/"
        ],
        "keywords": ["Spark", "Databricks", "Snowflake", "ETL", "ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯"],
        "difficulty": "ä¸­ç´š",
        "reading_time": "28åˆ†"
    }
]

def generate_ai_thought_process(topic_data):
    """AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’ç”Ÿæˆ"""
    
    references = topic_data["reference_sites"]
    keywords = topic_data["keywords"]
    category = CATEGORIES[topic_data["category"]]["name"]
    
    thought_process = f"""## ğŸ¤” ãªãœã“ã®è¨˜äº‹ã‚’æ›¸ã“ã†ã¨æ€ã£ãŸã®ã‹

æœ€è¿‘ã€æŠ€è¡“ç³»ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚„ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ã‚’è¦³å¯Ÿã—ã¦ã„ã¦ã€{keywords[0]}ã«é–¢ã™ã‚‹è­°è«–ãŒæ´»ç™ºã«ãªã£ã¦ã„ã‚‹ã“ã¨ã«æ°—ã¥ãã¾ã—ãŸã€‚

### å‚è€ƒã«ã—ãŸã‚µã‚¤ãƒˆã¨æ°—ã¥ã

#### 1. {references[0]}ã§ã®ç™ºè¦‹
ã“ã®ã‚µã‚¤ãƒˆã§{keywords[0]}é–¢é€£ã®æŠ•ç¨¿ã‚’è¦‹ã¦ã„ãŸã¨ã“ã‚ã€å¤šãã®é–‹ç™ºè€…ãŒ{keywords[1]}ã¨ã®é€£æºæ–¹æ³•ã«ã¤ã„ã¦æ‚©ã‚“ã§ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ç‰¹ã«åˆå¿ƒè€…ã®æ–¹ã€…ãŒã€Œã©ã“ã‹ã‚‰å§‹ã‚ã‚Œã°ã„ã„ã®ã‹ã€ã€Œã©ã‚“ãªè½ã¨ã—ç©´ãŒã‚ã‚‹ã®ã‹ã€ã¨ã„ã£ãŸè³ªå•ã‚’é »ç¹ã«ã—ã¦ã„ã‚‹ã®ã‚’ç›®ã«ã—ã¾ã—ãŸã€‚

#### 2. {references[1]}ã§ã®ãƒˆãƒ¬ãƒ³ãƒ‰
æœ€æ–°ã®æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¿½è·¡ã—ã¦ã„ã‚‹ã¨ã€{keywords[2]}ãŒæ€¥é€Ÿã«æ³¨ç›®ã‚’é›†ã‚ã¦ãŠã‚Šã€å®Ÿè£…ä¾‹ã‚„ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«å¯¾ã™ã‚‹éœ€è¦ãŒé«˜ã¾ã£ã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€ä½“ç³»çš„ã«ã¾ã¨ã‚ã‚‰ã‚ŒãŸæ—¥æœ¬èªã®è³‡æ–™ãŒã¾ã å°‘ãªã„ã“ã¨ã«æ°—ã¥ãã¾ã—ãŸã€‚

#### 3. {references[2]}ã§ã®è­°è«–
ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§ã®è­°è«–ã‚’è¦‹ã¦ã„ã‚‹ã¨ã€{keywords[3]}ã«é–¢ã™ã‚‹å®Ÿè·µçš„ãªçŸ¥è­˜ã¸ã®æ¸‡æœ›ãŒæ„Ÿã˜ã‚‰ã‚Œã¾ã—ãŸã€‚ç†è«–çš„ãªèª¬æ˜ã¯å¤šã„ã‚‚ã®ã®ã€å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ä½¿ãˆã‚‹å…·ä½“çš„ãªå®Ÿè£…ä¾‹ãŒä¸è¶³ã—ã¦ã„ã‚‹ã¨æ„Ÿã˜ã¾ã—ãŸã€‚

### è¨˜äº‹ã‚’æ›¸ãå‹•æ©Ÿ

ã“ã‚Œã‚‰ã®è¦³å¯Ÿã‹ã‚‰ã€ä»¥ä¸‹ã®ç‚¹ã‚’è§£æ±ºã™ã‚‹è¨˜äº‹ãŒå¿…è¦ã ã¨åˆ¤æ–­ã—ã¾ã—ãŸï¼š

1. **å®Ÿè·µçš„ãªå®Ÿè£…ä¾‹ã®ä¸è¶³** - ç†è«–ã ã‘ã§ãªãã€ã™ãã«ä½¿ãˆã‚‹ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’æä¾›ã™ã‚‹å¿…è¦æ€§
2. **æ®µéšçš„ãªå­¦ç¿’ãƒ‘ã‚¹** - åˆå¿ƒè€…ã‹ã‚‰ä¸Šç´šè€…ã¾ã§ã€ãã‚Œãã‚Œã®ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸå†…å®¹ã®æä¾›
3. **æœ€æ–°æƒ…å ±ã®çµ±åˆ** - æ•£åœ¨ã—ã¦ã„ã‚‹æƒ…å ±ã‚’ä¸€ã¤ã®è¨˜äº‹ã«ã¾ã¨ã‚ã€2025å¹´æ™‚ç‚¹ã§ã®æœ€æ–°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç¤ºã™
4. **æ—¥æœ¬èªã§ã®è©³ç´°ãªè§£èª¬** - è‹±èªã®è³‡æ–™ã¯è±Šå¯Œã ãŒã€æ—¥æœ¬èªã§ä¸å¯§ã«è§£èª¬ã•ã‚ŒãŸè³‡æ–™ã®å¿…è¦æ€§

ç‰¹ã«ã€{category}åˆ†é‡ã§ã¯æŠ€è¡“ã®é€²åŒ–ãŒé€Ÿãã€åŠå¹´å‰ã®æƒ…å ±ã§ã‚‚å¤ããªã£ã¦ã—ã¾ã†ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ã ã‹ã‚‰ã“ãã€ä»Šã“ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§æœ€æ–°ã®æƒ…å ±ã‚’ã¾ã¨ã‚ã€å®Ÿè·µçš„ãªçŸ¥è­˜ã¨ã—ã¦å…±æœ‰ã™ã‚‹ã“ã¨ã«ä¾¡å€¤ãŒã‚ã‚‹ã¨è€ƒãˆã¾ã—ãŸã€‚

ã“ã®è¨˜äº‹ã‚’é€šã˜ã¦ã€èª­è€…ã®çš†æ§˜ãŒ{keywords[0]}ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã—ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æˆåŠŸã«è²¢çŒ®ã§ãã‚‹ã“ã¨ã‚’é¡˜ã£ã¦ã„ã¾ã™ã€‚"""
    
    return thought_process

def generate_detailed_content(topic_data):
    """è©³ç´°ã§å……å®Ÿã—ãŸè¨˜äº‹å†…å®¹ã‚’ç”Ÿæˆï¼ˆã‚ˆã‚Šå¤šãã®è§£èª¬ã‚’å«ã‚€ï¼‰"""
    
    title = topic_data["title"]
    short_title = topic_data["short_title"]
    keywords = topic_data["keywords"]
    category = CATEGORIES[topic_data["category"]]
    difficulty = topic_data["difficulty"]
    reading_time = topic_data["reading_time"]
    
    # AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’ç”Ÿæˆ
    thought_process = generate_ai_thought_process(topic_data)
    
    content = f"""# {title}

**é›£æ˜“åº¦**: {difficulty} | **èª­äº†æ™‚é–“**: ç´„{reading_time} | **ã‚«ãƒ†ã‚´ãƒªãƒ¼**: {category['name']}

<details class="ai-thought-process">
<summary>ğŸ’­ AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹ï¼‰</summary>

{thought_process}

</details>

---

## ğŸ¯ ã“ã®è¨˜äº‹ã§å­¦ã¹ã‚‹ã“ã¨

ã“ã®è¨˜äº‹ã§ã¯ã€{short_title}ã«ã¤ã„ã¦ã€å®Ÿè·µçš„ãªè¦³ç‚¹ã‹ã‚‰è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ï¼š

- {keywords[0]}ã®åŸºæœ¬æ¦‚å¿µã¨æœ€æ–°å‹•å‘
- {keywords[1]}ã¨ã®é€£æºæ–¹æ³•ã¨å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
- å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®{keywords[2]}æ´»ç”¨äº‹ä¾‹
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã¨{keywords[3]}ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

## ğŸ“‹ ç›®æ¬¡

1. [ã¯ã˜ã‚ã«](#ã¯ã˜ã‚ã«)
2. [æŠ€è¡“çš„èƒŒæ™¯ã¨é‡è¦æ€§](#æŠ€è¡“çš„èƒŒæ™¯ã¨é‡è¦æ€§)
3. [åŸºæœ¬æ¦‚å¿µã®ç†è§£](#åŸºæœ¬æ¦‚å¿µã®ç†è§£)
4. [å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰](#å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰)
5. [å®Ÿè·µçš„ãªå¿œç”¨ä¾‹](#å®Ÿè·µçš„ãªå¿œç”¨ä¾‹)
6. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)
7. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
8. [ä»Šå¾Œã®å±•æœ›](#ä»Šå¾Œã®å±•æœ›)
9. [ã¾ã¨ã‚](#ã¾ã¨ã‚)

---

## ğŸŒŸ ã¯ã˜ã‚ã«

{datetime.now().year}å¹´ã€ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®é€²åŒ–ã¯åŠ é€Ÿåº¦çš„ã«é€²ã‚“ã§ã„ã¾ã™ã€‚ç‰¹ã«{short_title}ã®åˆ†é‡ã¯ã€ã“ã“æ•°ãƒ¶æœˆã§åŠ‡çš„ãªå¤‰åŒ–ã‚’é‚ã’ã¦ã„ã¾ã™ã€‚

### ãªãœä»Šã€{keywords[0]}ãŒé‡è¦ãªã®ã‹

ç¾ä»£ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã«ãŠã„ã¦ã€{keywords[0]}ã¯é¿ã‘ã¦é€šã‚Œãªã„æŠ€è¡“ã¨ãªã£ã¦ã„ã¾ã™ã€‚ãã®ç†ç”±ã‚’è©³ã—ãè¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚

#### 1. ç”Ÿç”£æ€§ã®åŠ‡çš„ãªå‘ä¸Š

å¾“æ¥ã®æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦ã€é–‹ç™ºé€Ÿåº¦ãŒ2ã€œ3å€å‘ä¸Šã™ã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯å˜ã«ä½œæ¥­ãŒé€Ÿããªã‚‹ã ã‘ã§ãªãã€ä»¥ä¸‹ã®ã‚ˆã†ãªè³ªçš„ãªå¤‰åŒ–ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ï¼š

- **è‡ªå‹•åŒ–ã«ã‚ˆã‚‹äººçš„ãƒŸã‚¹ã®å‰Šæ¸›**: ç¹°ã‚Šè¿”ã—ä½œæ¥­ã‚’AIã‚„ãƒ„ãƒ¼ãƒ«ã«ä»»ã›ã‚‹ã“ã¨ã§ã€äººé–“ã¯ã‚ˆã‚Šå‰µé€ çš„ãªä½œæ¥­ã«é›†ä¸­ã§ãã¾ã™
- **ä¸€è²«æ€§ã®ã‚ã‚‹å®Ÿè£…**: ãƒãƒ¼ãƒ å…¨ä½“ã§çµ±ä¸€ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚³ãƒ¼ãƒ‰ã®å“è³ªãŒå‘ä¸Šã—ã¾ã™
- **è¿…é€Ÿãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—**: å•é¡Œã‚’æ—©æœŸã«ç™ºè¦‹ã—ã€ä¿®æ­£ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™

#### 2. å“è³ªã®å‘ä¸Š

{keywords[0]}ã‚’é©åˆ‡ã«æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå“è³ªå‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™ï¼š

- **ãƒ†ã‚¹ãƒˆå¯èƒ½æ€§ã®å‘ä¸Š**: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸè¨­è¨ˆã«ã‚ˆã‚Šã€å˜ä½“ãƒ†ã‚¹ãƒˆãŒæ›¸ãã‚„ã™ããªã‚Šã¾ã™
- **ä¿å®ˆæ€§ã®å‘ä¸Š**: æ˜ç¢ºãªè²¬ä»»åˆ†é›¢ã«ã‚ˆã‚Šã€å°†æ¥ã®å¤‰æ›´ãŒå®¹æ˜“ã«ãªã‚Šã¾ã™
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è‡ªå‹•ç”Ÿæˆ**: ã‚³ãƒ¼ãƒ‰ã‹ã‚‰è‡ªå‹•çš„ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã§ãã¾ã™

#### 3. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

å°è¦æ¨¡ãªãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‹ã‚‰å¤§è¦æ¨¡ãªã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚·ã‚¹ãƒ†ãƒ ã¾ã§ã€åŒã˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å¯¾å¿œå¯èƒ½ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šï¼š

- **æ®µéšçš„ãªæˆé•·**: æœ€åˆã¯å°ã•ãå§‹ã‚ã¦ã€å¿…è¦ã«å¿œã˜ã¦æ‹¡å¼µã§ãã¾ã™
- **ãƒªã‚½ãƒ¼ã‚¹ã®æœ€é©åŒ–**: è² è·ã«å¿œã˜ã¦è‡ªå‹•çš„ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§ãã¾ã™
- **ã‚°ãƒ­ãƒ¼ãƒãƒ«å±•é–‹**: åœ°ç†çš„ã«åˆ†æ•£ã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‚‚å®¹æ˜“ã«æ§‹ç¯‰ã§ãã¾ã™

---

## ğŸ”§ æŠ€è¡“çš„èƒŒæ™¯ã¨é‡è¦æ€§

### å¾“æ¥ã®èª²é¡Œã¨ãã®è§£æ±ºç­–

ã“ã‚Œã¾ã§ã®é–‹ç™ºç¾å ´ã§ã¯ã€å¤šãã®èª²é¡ŒãŒå­˜åœ¨ã—ã¦ã„ã¾ã—ãŸã€‚ãã‚Œãã‚Œã®èª²é¡Œã¨ã€{keywords[0]}ã«ã‚ˆã‚‹è§£æ±ºç­–ã‚’è©³ã—ãè¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚

#### èª²é¡Œ1: æ‰‹å‹•ä½œæ¥­ã®å¤šã•

**å¾“æ¥ã®å•é¡Œç‚¹:**
- åŒã˜ã‚ˆã†ãªä½œæ¥­ã‚’ä½•åº¦ã‚‚ç¹°ã‚Šè¿”ã™å¿…è¦ãŒã‚ã£ãŸ
- ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã‚„ã™ã‹ã£ãŸ
- ä½œæ¥­ã®æ¨™æº–åŒ–ãŒå›°é›£ã ã£ãŸ

**{keywords[0]}ã«ã‚ˆã‚‹è§£æ±º:**

```python
# å¾“æ¥ã®æ–¹æ³•
def traditional_approach():
    """æ‰‹å‹•ã§ä¸€ã¤ãšã¤å‡¦ç†ã™ã‚‹å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"""
    results = []
    for item in data:
        # å„ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã—ã¦æ‰‹å‹•ã§å‡¦ç†ã‚’å®Ÿè¡Œ
        processed = manual_process(item)
        if validate(processed):
            results.append(processed)
        else:
            # ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚‚æ‰‹å‹•
            log_error(f"Failed to process {item}")
    return results

# {keywords[0]}ã‚’ä½¿ã£ãŸæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
async def modern_approach():
    """AIã¨è‡ªå‹•åŒ–ã‚’æ´»ç”¨ã—ãŸæœ€æ–°ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"""
    async with AIProcessor() as processor:
        # ãƒãƒƒãƒå‡¦ç†ã¨ä¸¦åˆ—å®Ÿè¡Œã§é«˜é€ŸåŒ–
        results = await processor.batch_process(
            data,
            optimization_level="high",
            auto_scale=True,
            error_handling="automatic"
        )
        
        # å‡¦ç†çµæœã®è‡ªå‹•æ¤œè¨¼
        validated_results = await processor.validate_results(
            results,
            validation_rules=get_validation_rules()
        )
        
        # å•é¡ŒãŒã‚ã‚Œã°è‡ªå‹•çš„ã«å†è©¦è¡Œ
        if validated_results.has_errors():
            fixed_results = await processor.auto_fix(
                validated_results.errors,
                max_retries=3
            )
            validated_results.merge(fixed_results)
            
    return validated_results
```

ã“ã®ã‚³ãƒ¼ãƒ‰ã®é•ã„ã‚’è©³ã—ãè§£èª¬ã™ã‚‹ã¨ï¼š

1. **éåŒæœŸå‡¦ç†ã®æ´»ç”¨**: `async/await`ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€I/Oå¾…æ©Ÿæ™‚é–“ã‚’æœ‰åŠ¹æ´»ç”¨
2. **ãƒãƒƒãƒå‡¦ç†**: è¤‡æ•°ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›
3. **è‡ªå‹•ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®å‡¦ç†ã‚’è‡ªå‹•åŒ–
4. **è‡ªå‹•æ¤œè¨¼**: çµæœã®å¦¥å½“æ€§ã‚’è‡ªå‹•çš„ã«ãƒã‚§ãƒƒã‚¯

#### èª²é¡Œ2: å±äººåŒ–

**å¾“æ¥ã®å•é¡Œç‚¹:**
- ç‰¹å®šã®é–‹ç™ºè€…ã—ã‹ç†è§£ã§ããªã„ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒä¸è¶³ã¾ãŸã¯å¤ã„
- çŸ¥è­˜ã®å…±æœ‰ãŒå›°é›£

**è§£æ±ºç­–ã®è©³ç´°:**

{keywords[0]}ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ¨™æº–åŒ–ã•ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¯èƒ½ã«ãªã‚Šã¾ã™ï¼š

```python
# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹æ¨™æº–åŒ–
# config.yaml
project_config:
  standards:
    code_style: "pep8"
    documentation: "sphinx"
    testing: "pytest"
  
  patterns:
    api_design: "rest"
    data_validation: "pydantic"
    error_handling: "structured"
    
  automation:
    ci_cd: "github_actions"
    deployment: "kubernetes"
    monitoring: "prometheus"
```

ã“ã®è¨­å®šã«ã‚ˆã‚Šã€ãƒãƒ¼ãƒ å…¨ä½“ã§çµ±ä¸€ã•ã‚ŒãŸé–‹ç™ºãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¶­æŒã§ãã¾ã™ã€‚

---

## ğŸ“š åŸºæœ¬æ¦‚å¿µã®ç†è§£

### 1. ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è©³ç´°è§£èª¬

{keywords[0]}ã‚·ã‚¹ãƒ†ãƒ ã¯ã€è¤‡æ•°ã®å±¤ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹æ´—ç·´ã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æŒã£ã¦ã„ã¾ã™ã€‚å„å±¤ã®å½¹å‰²ã¨ç›¸äº’ä½œç”¨ã‚’è©³ã—ãè¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚

#### a) ãƒ‡ãƒ¼ã‚¿å±¤ã®è©³ç´°

ãƒ‡ãƒ¼ã‚¿å±¤ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ã®åŸºç›¤ã¨ãªã‚‹é‡è¦ãªéƒ¨åˆ†ã§ã™ã€‚ä»¥ä¸‹ã®è¦ç´ ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š

**å…¥åŠ›å‡¦ç†ã‚µãƒ–ã‚·ã‚¹ãƒ†ãƒ :**

```python
class DataInputHandler:
    """ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã‚’å‡¦ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.validators = self._setup_validators()
        self.transformers = self._setup_transformers()
        
    async def process_input(self, raw_data: Any) -> ProcessedData:
        """
        ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦ã€ã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨å¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        
        å‡¦ç†ã®æµã‚Œï¼š
        1. ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®æ¤œè¨¼
        2. å¿…è¦ãªå¤‰æ›ã®é©ç”¨
        3. æ­£è¦åŒ–ã¨ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä»˜ä¸
        """
        # Step 1: å½¢å¼æ¤œè¨¼
        validation_result = await self._validate_format(raw_data)
        if not validation_result.is_valid:
            raise InvalidDataFormatError(
                f"ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒä¸æ­£ã§ã™: {validation_result.errors}"
            )
            
        # Step 2: ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        transformed_data = await self._transform_data(
            raw_data,
            source_format=validation_result.detected_format,
            target_format=self.config['target_format']
        )
        
        # Step 3: ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        cleaned_data = await self._clean_data(transformed_data)
        
        # Step 4: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ä¸
        return ProcessedData(
            data=cleaned_data,
            metadata={
                'processed_at': datetime.now(),
                'source_format': validation_result.detected_format,
                'quality_score': await self._calculate_quality_score(cleaned_data)
            }
        )
```

ã“ã®ã‚³ãƒ¼ãƒ‰ã®å„éƒ¨åˆ†ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¾ã™ï¼š

1. **æ¤œè¨¼ãƒ—ãƒ­ã‚»ã‚¹**: ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã‚’è‡ªå‹•çš„ã«æ¤œå‡ºã—ã€æœŸå¾…ã•ã‚Œã‚‹å½¢å¼ã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
2. **å¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹**: ç•°ãªã‚‹å½¢å¼é–“ã§ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚’æŸ”è»Ÿã«å®Ÿè¡Œ
3. **ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°**: ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã®é™¤å»ã€æ¬ æå€¤ã®å‡¦ç†ã€ç•°å¸¸å€¤ã®æ¤œå‡º
4. **å“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°**: ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’æ•°å€¤åŒ–ã—ã€å¾Œç¶šã®å‡¦ç†ã§æ´»ç”¨

**å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³:**

å‰å‡¦ç†ã¯ã€ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æå¯èƒ½ãªå½¢å¼ã«å¤‰æ›ã™ã‚‹é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã§ã™ï¼š

```python
class PreprocessingPipeline:
    """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self):
        self.stages = []
        self._setup_default_stages()
        
    def _setup_default_stages(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‰å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’è¨­å®š"""
        self.add_stage('normalize', NormalizationStage())
        self.add_stage('encode', EncodingStage())
        self.add_stage('feature_extract', FeatureExtractionStage())
        self.add_stage('augment', DataAugmentationStage())
        
    async def execute(self, data: ProcessedData) -> EnrichedData:
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ enriched å½¢å¼ã«å¤‰æ›
        
        å„ã‚¹ãƒ†ãƒ¼ã‚¸ã§ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œï¼š
        - ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€æ¨™æº–åŒ–ãªã©ï¼‰
        - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        - ç‰¹å¾´é‡ã®æŠ½å‡ºã¨é¸æŠ
        - ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        """
        current_data = data
        
        for stage_name, stage in self.stages:
            try:
                # å„ã‚¹ãƒ†ãƒ¼ã‚¸ã®å®Ÿè¡Œã¨é€²æ—ç›£è¦–
                current_data = await stage.process(current_data)
                
                # ã‚¹ãƒ†ãƒ¼ã‚¸é–“ã®æ¤œè¨¼
                self._validate_stage_output(stage_name, current_data)
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨˜éŒ²
                self._record_metrics(stage_name, current_data)
                
            except StageProcessingError as e:
                # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®è©³ç´°ãªãƒ­ã‚°ã¨å›å¾©å‡¦ç†
                logger.error(f"ã‚¹ãƒ†ãƒ¼ã‚¸ {stage_name} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                current_data = await self._recover_from_error(
                    stage_name, current_data, e
                )
                
        return EnrichedData(current_data)
```

#### b) å‡¦ç†å±¤ã®è©³ç´°

å‡¦ç†å±¤ã¯ã€å®Ÿéš›ã®è¨ˆç®—ã¨ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ä¸­æ ¸éƒ¨åˆ†ã§ã™ï¼š

**ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã®å®Ÿè£…:**

```python
class ParallelProcessingEngine:
    """é«˜æ€§èƒ½ãªä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.worker_pool = self._create_worker_pool()
        self.task_queue = asyncio.Queue()
        self.result_queue = asyncio.Queue()
        
    async def process_batch(self, items: List[Any]) -> List[Result]:
        """
        ãƒãƒƒãƒå‡¦ç†ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        
        æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆï¼š
        1. å‹•çš„ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®èª¿æ•´
        2. ã‚¿ã‚¹ã‚¯ã®å„ªå…ˆåº¦ä»˜ã‘
        3. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
        4. å‡¦ç†æ™‚é–“ã®äºˆæ¸¬ã¨æœ€é©åŒ–
        """
        # ãƒãƒƒãƒã‚’æœ€é©ãªã‚µã‚¤ã‚ºã«åˆ†å‰²
        optimal_chunks = self._split_into_optimal_chunks(items)
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ
        tasks = []
        for priority, chunk in enumerate(optimal_chunks):
            task = ProcessingTask(
                data=chunk,
                priority=self._calculate_priority(chunk),
                estimated_time=self._estimate_processing_time(chunk)
            )
            tasks.append(task)
            
        # ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        for task in sorted(tasks, key=lambda t: t.priority, reverse=True):
            await self.task_queue.put(task)
            
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ã‚ˆã‚‹ä¸¦åˆ—å‡¦ç†
        results = await self._execute_parallel_processing(len(tasks))
        
        # çµæœã®é›†ç´„ã¨å¾Œå‡¦ç†
        return await self._aggregate_results(results)
        
    def _calculate_priority(self, chunk: List[Any]) -> float:
        """
        ãƒãƒ£ãƒ³ã‚¯ã®å„ªå…ˆåº¦ã‚’è¨ˆç®—
        
        è€ƒæ…®ã™ã‚‹è¦ç´ ï¼š
        - ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
        - å‡¦ç†ã®è¤‡é›‘ã•
        - ãƒ“ã‚¸ãƒã‚¹ä¸Šã®é‡è¦åº¦
        - ç· åˆ‡æ™‚é–“
        """
        size_factor = len(chunk) / self.config.average_chunk_size
        complexity_factor = self._estimate_complexity(chunk)
        importance_factor = self._get_business_importance(chunk)
        urgency_factor = self._calculate_urgency(chunk)
        
        # é‡ã¿ä»˜ã‘ã—ã¦ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        priority = (
            size_factor * 0.2 +
            complexity_factor * 0.3 +
            importance_factor * 0.3 +
            urgency_factor * 0.2
        )
        
        return priority
```

**æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©³ç´°:**

```python
class OptimizationAlgorithm:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ """
    
    def __init__(self):
        self.performance_history = deque(maxlen=1000)
        self.optimization_model = self._build_optimization_model()
        
    async def optimize_processing(self, current_state: SystemState) -> OptimizationPlan:
        """
        ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’åˆ†æã—ã€æœ€é©åŒ–è¨ˆç”»ã‚’ç”Ÿæˆ
        
        æœ€é©åŒ–ã®è¦³ç‚¹ï¼š
        1. ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨åŠ¹ç‡
        2. å‡¦ç†æ™‚é–“
        3. ã‚¨ãƒ©ãƒ¼ç‡
        4. ã‚³ã‚¹ãƒˆåŠ¹ç‡
        """
        # ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†
        metrics = await self._collect_performance_metrics(current_state)
        
        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã¨çµ„ã¿åˆã‚ã›ã¦åˆ†æ
        analysis_result = self._analyze_performance_trends(
            current_metrics=metrics,
            historical_data=self.performance_history
        )
        
        # æœ€é©åŒ–ã®æ©Ÿä¼šã‚’ç‰¹å®š
        optimization_opportunities = self._identify_optimization_opportunities(
            analysis_result
        )
        
        # å„æœ€é©åŒ–æ¡ˆã®å½±éŸ¿ã‚’äºˆæ¸¬
        optimization_plans = []
        for opportunity in optimization_opportunities:
            plan = await self._create_optimization_plan(opportunity)
            impact = await self._predict_impact(plan, current_state)
            
            if impact.expected_improvement > self.config.min_improvement_threshold:
                optimization_plans.append({
                    'plan': plan,
                    'impact': impact,
                    'risk_score': self._calculate_risk(plan)
                })
                
        # æœ€é©ãªè¨ˆç”»ã‚’é¸æŠ
        best_plan = self._select_best_plan(
            optimization_plans,
            risk_tolerance=self.config.risk_tolerance
        )
        
        return best_plan
```

---

## ğŸš€ å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

### Step 1: ç’°å¢ƒæ§‹ç¯‰ã®è©³ç´°

é–‹ç™ºç’°å¢ƒã®æ§‹ç¯‰ã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æˆåŠŸã®ç¬¬ä¸€æ­©ã§ã™ã€‚ä»¥ä¸‹ã€è©³ç´°ãªæ‰‹é †ã‚’èª¬æ˜ã—ã¾ã™ï¼š

#### 1.1 å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Pythonç’°å¢ƒã®æº–å‚™ï¼ˆæ¨å¥¨: Python 3.9ä»¥ä¸Šï¼‰
python --version  # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª

# ä»®æƒ³ç’°å¢ƒã®ä½œæˆï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆ†é›¢ï¼‰
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ã¾ãŸã¯
venv\\Scripts\\activate  # Windows

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install --upgrade pip  # pipè‡ªä½“ã‚’æœ€æ–°ã«
pip install {keywords[0].lower()}-toolkit
pip install -r requirements.txt

# é–‹ç™ºãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -e ".[dev]"  # é–‹ç™ºç”¨ä¾å­˜é–¢ä¿‚ã‚’å«ã‚€

# Node.jsé–¢é€£ãƒ„ãƒ¼ãƒ«ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰é–‹ç™ºã®å ´åˆï¼‰
npm install -g @{keywords[0].lower()}/cli
npm install -g typescript  # TypeScript ã‚µãƒãƒ¼ãƒˆ
```

#### 1.2 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir my-{keywords[0].lower()}-project
cd my-{keywords[0].lower()}-project

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–
{keywords[0].lower()} init

# å¯¾è©±å½¢å¼ã§ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š
# - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
# - ä½¿ç”¨ã™ã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
# - è¿½åŠ æ©Ÿèƒ½ã®é¸æŠ
# - ä¾å­˜é–¢ä¿‚ã®ç®¡ç†æ–¹æ³•
```

#### 1.3 IDE/ã‚¨ãƒ‡ã‚£ã‚¿ã®è¨­å®š

é–‹ç™ºåŠ¹ç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã€é©åˆ‡ãªIDEè¨­å®šãŒé‡è¦ã§ã™ï¼š

```json
// .vscode/settings.json
{
    "python.linting.enabled": true,
    "python.linting.pylintEnabled": true,
    "python.formatting.provider": "black",
    "python.testing.pytestEnabled": true,
    "editor.formatOnSave": true,
    "[python]": {
        "editor.rulers": [88],
        "editor.codeActionsOnSave": {
            "source.organizeImports": true
        }
    },
    "{keywords[0].lower()}.autocomplete": true,
    "{keywords[0].lower()}.validation": true
}
```

### Step 2: åŸºæœ¬è¨­å®šã®è©³ç´°è§£èª¬

è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å‹•ä½œã‚’åˆ¶å¾¡ã™ã‚‹é‡è¦ãªè¦ç´ ã§ã™ï¼š

```python
# config.py
from {keywords[0].lower()} import Config, Environment
from typing import Dict, Any
import os

class ProjectConfig:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, env: str = None):
        self.env = env or os.getenv('ENVIRONMENT', 'development')
        self.config = self._load_config()
        
    def _load_config(self) -> Config:
        """ç’°å¢ƒã«å¿œã˜ãŸè¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰"""
        
        # åŸºæœ¬è¨­å®š
        base_config = {
            'project_name': 'my-awesome-project',
            'version': '1.0.0',
            'author': 'Your Name',
            'description': '{keywords[0]}ã‚’ä½¿ã£ãŸé©æ–°çš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ'
        }
        
        # ç’°å¢ƒåˆ¥è¨­å®š
        env_configs = {
            'development': {
                'debug': True,
                'log_level': 'DEBUG',
                'cache_enabled': False,
                'hot_reload': True
            },
            'staging': {
                'debug': False,
                'log_level': 'INFO',
                'cache_enabled': True,
                'hot_reload': False
            },
            'production': {
                'debug': False,
                'log_level': 'WARNING',
                'cache_enabled': True,
                'hot_reload': False,
                'performance_monitoring': True
            }
        }
        
        # æ©Ÿèƒ½è¨­å®š
        features = {
            'auto_scaling': {
                'enabled': self.env == 'production',
                'min_instances': 2,
                'max_instances': 10,
                'cpu_threshold': 70,
                'memory_threshold': 80,
                'scale_up_cooldown': 60,  # ç§’
                'scale_down_cooldown': 300  # ç§’
            },
            'monitoring': {
                'enabled': True,
                'metrics_interval': 60,  # ç§’
                'alert_channels': ['email', 'slack'],
                'custom_metrics': {
                    'business_metrics': True,
                    'performance_metrics': True,
                    'error_metrics': True
                }
            },
            'caching': {
                'enabled': env_configs[self.env]['cache_enabled'],
                'ttl': 3600,  # ç§’
                'max_size': '1GB',
                'eviction_policy': 'LRU',
                'distributed': self.env == 'production'
            },
            'security': {
                'encryption': {
                    'algorithm': 'AES-256-GCM',
                    'key_rotation': True,
                    'rotation_interval': 2592000  # 30æ—¥
                },
                'authentication': {
                    'method': 'OAuth2',
                    'providers': ['google', 'github', 'custom'],
                    'session_timeout': 3600,
                    'mfa_enabled': self.env == 'production'
                },
                'rate_limiting': {
                    'enabled': True,
                    'default_limit': 100,  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†
                    'burst_limit': 150,
                    'custom_limits': {
                        '/api/heavy-operation': 10,
                        '/api/auth/*': 20
                    }
                }
            }
        }
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
        performance = {
            'max_workers': os.cpu_count() or 4,
            'thread_pool_size': 10,
            'connection_pool_size': 20,
            'timeout': {
                'default': 30,
                'long_running': 300,
                'database': 10
            },
            'retry': {
                'max_attempts': 3,
                'backoff_factor': 2,
                'max_backoff': 60
            },
            'batch_processing': {
                'enabled': True,
                'batch_size': 100,
                'max_batch_size': 1000,
                'parallel_batches': 4
            }
        }
        
        # è¨­å®šã‚’çµ±åˆ
        return Config(
            environment=Environment[self.env.upper()],
            base=base_config,
            env_specific=env_configs[self.env],
            features=features,
            performance=performance
        )
    
    def get(self, key: str, default: Any = None) -> Any:
        """è¨­å®šå€¤ã‚’å–å¾—"""
        return self.config.get(key, default)
        
    def update(self, updates: Dict[str, Any]) -> None:
        """è¨­å®šã‚’å‹•çš„ã«æ›´æ–°"""
        self.config.update(updates)
        self._validate_config()
        
    def _validate_config(self) -> None:
        """è¨­å®šã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
        # å¿…é ˆé …ç›®ã®ãƒã‚§ãƒƒã‚¯
        required_keys = ['project_name', 'version', 'environment']
        for key in required_keys:
            if key not in self.config:
                raise ValueError(f"å¿…é ˆè¨­å®šé …ç›® '{key}' ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                
        # å€¤ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if self.config['performance']['max_workers'] < 1:
            raise ValueError("max_workers ã¯1ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
            
        # ãã®ä»–ã®ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ãæ¤œè¨¼
        self._validate_business_rules()
```

---

## ğŸ’¡ å®Ÿè·µçš„ãªå¿œç”¨ä¾‹

### ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹1: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰

å¤§é‡ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹å®Ÿè·µçš„ãªä¾‹ã‚’è©³ã—ãè¦‹ã¦ã„ãã¾ã—ã‚‡ã†ï¼š

#### ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã®æ¦‚è¦

```python
# realtime_system.py
from typing import Dict, List, Any, Optional
import asyncio
import websockets
from collections import defaultdict
from datetime import datetime, timedelta
import json

class RealtimeDataProcessingSystem:
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
    
    ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã¡ã¾ã™ï¼š
    1. é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆ10,000+ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸/ç§’ï¼‰
    2. ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆ< 100msï¼‰
    3. é«˜å¯ç”¨æ€§ï¼ˆ99.9% ã‚¢ãƒƒãƒ—ã‚¿ã‚¤ãƒ ï¼‰
    4. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ï¼ˆæ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œï¼‰
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.processor = self._create_processor()
        self.metrics_collector = MetricsCollector()
        self.error_handler = ErrorHandler(config['error_handling'])
        self.state_manager = StateManager()
        
    def _create_processor(self) -> StreamProcessor:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ä½œæˆ"""
        return StreamProcessor(
            buffer_size=self.config.get('buffer_size', 10000),
            flush_interval=self.config.get('flush_interval', 1.0),
            parallel_workers=self.config.get('parallel_workers', 8),
            backpressure_strategy=self.config.get('backpressure', 'adaptive')
        )
        
    async def start_processing(self, websocket_urls: List[str]):
        """
        è¤‡æ•°ã®WebSocketã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡ã—ã¦å‡¦ç†
        
        Args:
            websocket_urls: æ¥ç¶šå…ˆã®WebSocket URLãƒªã‚¹ãƒˆ
        """
        # å„URLã«å¯¾ã—ã¦ç‹¬ç«‹ã—ãŸã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¢ºç«‹
        tasks = []
        for url in websocket_urls:
            task = asyncio.create_task(
                self._process_stream(url),
                name=f"stream_{url}"
            )
            tasks.append(task)
            
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        metrics_task = asyncio.create_task(
            self._collect_metrics(),
            name="metrics_collector"
        )
        tasks.append(metrics_task)
        
        # çŠ¶æ…‹ç®¡ç†ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        state_task = asyncio.create_task(
            self._manage_state(),
            name="state_manager"
        )
        tasks.append(state_task)
        
        try:
            # ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸¦è¡Œå®Ÿè¡Œ
            await asyncio.gather(*tasks)
        except Exception as e:
            self.error_handler.handle_critical_error(e)
            await self._graceful_shutdown()
            
    async def _process_stream(self, url: str):
        """
        å€‹åˆ¥ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡¦ç†
        
        å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«å¯¾ã—ã¦ï¼š
        1. æ¥ç¶šã®ç¢ºç«‹ã¨ç¶­æŒ
        2. ãƒ‡ãƒ¼ã‚¿ã®å—ä¿¡ã¨ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°
        3. ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè¡Œ
        4. çµæœã®é…ä¿¡
        """
        reconnect_attempts = 0
        max_reconnect_attempts = self.config.get('max_reconnect_attempts', 5)
        
        while reconnect_attempts < max_reconnect_attempts:
            try:
                async with websockets.connect(url) as websocket:
                    reconnect_attempts = 0  # æ¥ç¶šæˆåŠŸã§ãƒªã‚»ãƒƒãƒˆ
                    
                    async for message in websocket:
                        try:
                            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è§£æ
                            data = self._parse_message(message)
                            
                            # ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
                            if not self._validate_data(data):
                                self.metrics_collector.increment('invalid_messages')
                                continue
                                
                            # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã«è¿½åŠ 
                            await self.processor.add(data)
                            
                            # ãƒãƒƒãƒ•ã‚¡ãŒé–¾å€¤ã«é”ã—ãŸã‚‰å‡¦ç†
                            if self.processor.should_flush():
                                await self._flush_and_process()
                                
                        except json.JSONDecodeError as e:
                            self.error_handler.handle_parse_error(e, message)
                        except Exception as e:
                            self.error_handler.handle_processing_error(e, data)
                            
            except websockets.exceptions.ConnectionClosed:
                reconnect_attempts += 1
                wait_time = self._calculate_backoff(reconnect_attempts)
                logger.warning(
                    f"æ¥ç¶šãŒåˆ‡æ–­ã•ã‚Œã¾ã—ãŸã€‚{wait_time}ç§’å¾Œã«å†æ¥ç¶šã‚’è©¦ã¿ã¾ã™ã€‚"
                    f"(è©¦è¡Œ: {reconnect_attempts}/{max_reconnect_attempts})"
                )
                await asyncio.sleep(wait_time)
                
            except Exception as e:
                self.error_handler.handle_connection_error(e, url)
                reconnect_attempts += 1
                
    def _parse_message(self, message: str) -> Dict[str, Any]:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è§£æã—ã¦ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¤‰æ›
        
        å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼š
        - JSON
        - MessagePack
        - Protocol Buffers
        - ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        """
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•æ¤œå‡º
        message_type = self._detect_message_type(message)
        
        parser = self._get_parser(message_type)
        parsed_data = parser.parse(message)
        
        # å…±é€šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
        parsed_data['_metadata'] = {
            'received_at': datetime.now(),
            'message_type': message_type,
            'size_bytes': len(message)
        }
        
        return parsed_data
        
    async def _flush_and_process(self):
        """
        ãƒãƒƒãƒ•ã‚¡å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã—ã¦å‡¦ç†
        """
        # ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        batch = await self.processor.flush()
        
        if not batch:
            return
            
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨˜éŒ²
        self.metrics_collector.record('batch_size', len(batch))
        
        # å‡¦ç†é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
        start_time = asyncio.get_event_loop().time()
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
            preprocessed = await self._preprocess_batch(batch)
            
            # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ
            results = await self._execute_main_processing(preprocessed)
            
            # å¾Œå‡¦ç†
            final_results = await self._postprocess_results(results)
            
            # çµæœã®é…ä¿¡
            await self._deliver_results(final_results)
            
            # å‡¦ç†æ™‚é–“ã‚’è¨˜éŒ²
            processing_time = asyncio.get_event_loop().time() - start_time
            self.metrics_collector.record('processing_time', processing_time)
            
            # æˆåŠŸç‡ã‚’æ›´æ–°
            self.metrics_collector.increment('successful_batches')
            
        except Exception as e:
            self.error_handler.handle_batch_error(e, batch)
            self.metrics_collector.increment('failed_batches')
            
    async def _preprocess_batch(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ãƒãƒƒãƒã®å‰å‡¦ç†
        
        å®Ÿè¡Œã™ã‚‹å‡¦ç†ï¼š
        1. ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
        2. é‡è¤‡ã®é™¤å»
        3. ãƒ‡ãƒ¼ã‚¿ã®é›†ç´„
        4. ç‰¹å¾´é‡ã®æŠ½å‡º
        """
        # é‡è¤‡é™¤å»
        unique_batch = self._remove_duplicates(batch)
        
        # ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
        normalized_batch = []
        for item in unique_batch:
            normalized = await self._normalize_data(item)
            normalized_batch.append(normalized)
            
        # é–¢é€£ãƒ‡ãƒ¼ã‚¿ã®é›†ç´„
        aggregated_batch = await self._aggregate_related_data(normalized_batch)
        
        # ç‰¹å¾´é‡ã®æŠ½å‡º
        enriched_batch = []
        for item in aggregated_batch:
            features = await self._extract_features(item)
            item['features'] = features
            enriched_batch.append(item)
            
        return enriched_batch
        
    def _remove_duplicates(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»
        
        é‡è¤‡åˆ¤å®šã®åŸºæº–ï¼š
        - ä¸€æ„è­˜åˆ¥å­ï¼ˆIDï¼‰
        - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨å†…å®¹ã®ãƒãƒƒã‚·ãƒ¥
        - ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã«åŸºã¥ãé‡è¤‡åˆ¤å®š
        """
        seen = set()
        unique_items = []
        
        for item in batch:
            # é‡è¤‡åˆ¤å®šã‚­ãƒ¼ã‚’ç”Ÿæˆ
            dedup_key = self._generate_dedup_key(item)
            
            if dedup_key not in seen:
                seen.add(dedup_key)
                unique_items.append(item)
            else:
                self.metrics_collector.increment('duplicates_removed')
                
        return unique_items
```

#### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®å®Ÿè£…

```python
class PerformanceOptimizer:
    """
    ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‹•çš„ã«æœ€é©åŒ–
    """
    
    def __init__(self, system: RealtimeDataProcessingSystem):
        self.system = system
        self.performance_history = deque(maxlen=1000)
        self.optimization_interval = 60  # ç§’
        self.last_optimization = datetime.now()
        
    async def optimize_continuously(self):
        """
        ç¶™ç¶šçš„ãªæœ€é©åŒ–ã‚’å®Ÿè¡Œ
        """
        while True:
            try:
                # æœ€é©åŒ–é–“éš”ã¾ã§å¾…æ©Ÿ
                await asyncio.sleep(self.optimization_interval)
                
                # ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†
                current_metrics = await self._collect_current_metrics()
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ã‚’æ›´æ–°
                self.performance_history.append({
                    'timestamp': datetime.now(),
                    'metrics': current_metrics
                })
                
                # æœ€é©åŒ–ãŒå¿…è¦ã‹åˆ¤å®š
                if self._should_optimize(current_metrics):
                    optimization_plan = await self._create_optimization_plan(
                        current_metrics
                    )
                    await self._apply_optimization(optimization_plan)
                    
            except Exception as e:
                logger.error(f"æœ€é©åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
                
    def _should_optimize(self, metrics: Dict[str, float]) -> bool:
        """
        æœ€é©åŒ–ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        
        åˆ¤å®šåŸºæº–ï¼š
        - CPUä½¿ç”¨ç‡ãŒé–¾å€¤ã‚’è¶…ãˆã¦ã„ã‚‹
        - ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„
        - ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒç›®æ¨™å€¤ã‚’è¶…ãˆã¦ã„ã‚‹
        - ã‚¨ãƒ©ãƒ¼ç‡ãŒè¨±å®¹ç¯„å›²ã‚’è¶…ãˆã¦ã„ã‚‹
        """
        thresholds = {
            'cpu_usage': 80,  # %
            'memory_usage': 85,  # %
            'latency_p99': 100,  # ms
            'error_rate': 1  # %
        }
        
        for metric, threshold in thresholds.items():
            if metrics.get(metric, 0) > threshold:
                logger.info(f"æœ€é©åŒ–ãƒˆãƒªã‚¬ãƒ¼: {metric} = {metrics[metric]} > {threshold}")
                return True
                
        return False
        
    async def _create_optimization_plan(self, metrics: Dict[str, float]) -> Dict[str, Any]:
        """
        ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŸºã¥ã„ã¦æœ€é©åŒ–è¨ˆç”»ã‚’ä½œæˆ
        """
        plan = {
            'actions': [],
            'estimated_impact': {},
            'risk_level': 'low'
        }
        
        # CPUä½¿ç”¨ç‡ãŒé«˜ã„å ´åˆ
        if metrics.get('cpu_usage', 0) > 80:
            plan['actions'].append({
                'type': 'scale_workers',
                'action': 'increase',
                'target': min(
                    self.system.processor.parallel_workers + 2,
                    self.system.config['max_workers']
                )
            })
            plan['estimated_impact']['cpu_reduction'] = 20
            
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„å ´åˆ
        if metrics.get('memory_usage', 0) > 85:
            plan['actions'].append({
                'type': 'adjust_buffer',
                'action': 'reduce',
                'target': int(self.system.processor.buffer_size * 0.8)
            })
            plan['actions'].append({
                'type': 'flush_interval',
                'action': 'decrease',
                'target': max(0.5, self.system.processor.flush_interval * 0.8)
            })
            plan['estimated_impact']['memory_reduction'] = 15
            
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒé«˜ã„å ´åˆ
        if metrics.get('latency_p99', 0) > 100:
            plan['actions'].append({
                'type': 'batch_size',
                'action': 'decrease',
                'target': max(50, int(self.system.config['batch_size'] * 0.7))
            })
            plan['estimated_impact']['latency_reduction'] = 30
            plan['risk_level'] = 'medium'
            
        return plan
```

---

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. é«˜åº¦ãªã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

åŠ¹ç‡çš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è¦ã§ã™ï¼š

```python
class AdvancedCachingSystem:
    """
    å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # L1: ãƒ—ãƒ­ã‚»ã‚¹å†…ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.l1_cache = LRUCache(maxsize=config.get('l1_size', 1000))
        
        # L2: Redisåˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.l2_cache = redis.Redis(
            host=config['redis_host'],
            port=config['redis_port'],
            db=config['redis_db'],
            decode_responses=True,
            connection_pool_kwargs={
                'max_connections': 50,
                'socket_keepalive': True
            }
        )
        
        # L3: æ°¸ç¶šåŒ–å±¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        self.l3_cache = None
        if config.get('enable_persistent_cache'):
            self.l3_cache = PersistentCache(config['persistent_cache_path'])
            
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
        self.stats = CacheStatistics()
        
    async def get(self, key: str, compute_fn=None) -> Any:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å€¤ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚ã¯è¨ˆç®—ï¼‰
        
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ï¼š
        1. L1ï¼ˆãƒ¡ãƒ¢ãƒªï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
        2. L2ï¼ˆRedisï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
        3. L3ï¼ˆæ°¸ç¶šåŒ–å±¤ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
        4. ã™ã¹ã¦ãƒŸã‚¹ã®å ´åˆã¯è¨ˆç®—ã—ã¦å„å±¤ã«ä¿å­˜
        """
        # L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
        value = self.l1_cache.get(key)
        if value is not None:
            self.stats.record_hit('l1')
            return value
            
        # L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
        try:
            value = await self._get_from_l2(key)
            if value is not None:
                self.stats.record_hit('l2')
                # L1ã«æ˜‡æ ¼
                self.l1_cache.put(key, value)
                return value
        except redis.RedisError as e:
            logger.warning(f"L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ©ãƒ¼: {e}")
            
        # L3ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
        if self.l3_cache:
            value = await self.l3_cache.get(key)
            if value is not None:
                self.stats.record_hit('l3')
                # L1ã¨L2ã«æ˜‡æ ¼
                await self._promote_to_upper_layers(key, value)
                return value
                
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ - å€¤ã‚’è¨ˆç®—
        self.stats.record_miss()
        
        if compute_fn is None:
            return None
            
        # è¨ˆç®—ã‚’å®Ÿè¡Œ
        value = await compute_fn(key)
        
        # ã™ã¹ã¦ã®å±¤ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        await self._cache_to_all_layers(key, value)
        
        return value
        
    async def _get_from_l2(self, key: str) -> Any:
        """L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰éåŒæœŸã§å–å¾—"""
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ã¦åŠ¹ç‡åŒ–
        pipe = self.l2_cache.pipeline()
        pipe.get(key)
        pipe.ttl(key)
        
        results = await asyncio.to_thread(pipe.execute)
        value, ttl = results
        
        if value is not None and ttl > 0:
            # TTLãŒçŸ­ã„å ´åˆã¯äº‹å‰æ›´æ–°ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
            if ttl < self.config.get('refresh_threshold', 300):
                asyncio.create_task(self._schedule_refresh(key))
                
            return json.loads(value)
            
        return None
        
    async def _cache_to_all_layers(self, key: str, value: Any):
        """ã™ã¹ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±¤ã«å€¤ã‚’ä¿å­˜"""
        # L1ã«ä¿å­˜
        self.l1_cache.put(key, value)
        
        # L2ã«ä¿å­˜ï¼ˆéåŒæœŸï¼‰
        asyncio.create_task(self._save_to_l2(key, value))
        
        # L3ã«ä¿å­˜ï¼ˆéåŒæœŸï¼‰
        if self.l3_cache:
            asyncio.create_task(self.l3_cache.put(key, value))
            
    async def invalidate(self, pattern: str):
        """
        ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–
        
        Args:
            pattern: ç„¡åŠ¹åŒ–ã™ã‚‹ã‚­ãƒ¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰å¯¾å¿œï¼‰
        """
        # L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç„¡åŠ¹åŒ–
        invalidated_count = self.l1_cache.invalidate_pattern(pattern)
        
        # L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç„¡åŠ¹åŒ–
        if '*' in pattern:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            cursor = 0
            while True:
                cursor, keys = await asyncio.to_thread(
                    self.l2_cache.scan,
                    cursor,
                    match=pattern,
                    count=100
                )
                
                if keys:
                    await asyncio.to_thread(self.l2_cache.delete, *keys)
                    invalidated_count += len(keys)
                    
                if cursor == 0:
                    break
        else:
            # å®Œå…¨ä¸€è‡´
            deleted = await asyncio.to_thread(self.l2_cache.delete, pattern)
            invalidated_count += deleted
            
        # L3ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç„¡åŠ¹åŒ–
        if self.l3_cache:
            l3_count = await self.l3_cache.invalidate_pattern(pattern)
            invalidated_count += l3_count
            
        logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–å®Œäº†: {invalidated_count}ä»¶")
        return invalidated_count
```

### 2. GPU ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®æ´»ç”¨

è¨ˆç®—é›†ç´„çš„ãªã‚¿ã‚¹ã‚¯ã«GPUã‚’æ´»ç”¨ã™ã‚‹å®Ÿè£…ä¾‹ï¼š

```python
class GPUAcceleratedProcessor:
    """
    GPUã‚’æ´»ç”¨ã—ãŸé«˜é€Ÿãƒ‡ãƒ¼ã‚¿å‡¦ç†
    """
    
    def __init__(self):
        self.device = self._setup_device()
        self.memory_pool = self._create_memory_pool()
        self.stream_pool = self._create_stream_pool()
        
    def _setup_device(self) -> torch.device:
        """åˆ©ç”¨å¯èƒ½ãªæœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š"""
        if torch.cuda.is_available():
            # è¤‡æ•°GPUã®å ´åˆã¯æœ€ã‚‚ç©ºã„ã¦ã„ã‚‹ã‚‚ã®ã‚’é¸æŠ
            device_count = torch.cuda.device_count()
            if device_count > 1:
                # å„GPUã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª
                min_memory_used = float('inf')
                best_device = 0
                
                for i in range(device_count):
                    memory_used = torch.cuda.memory_allocated(i)
                    if memory_used < min_memory_used:
                        min_memory_used = memory_used
                        best_device = i
                        
                device = torch.device(f'cuda:{best_device}')
                logger.info(f"GPU {best_device} ã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆ{device_count}å°ä¸­ï¼‰")
            else:
                device = torch.device('cuda:0')
                logger.info("å˜ä¸€GPUã‚’ä½¿ç”¨ã—ã¾ã™")
                
            # GPUã®è©³ç´°æƒ…å ±ã‚’ãƒ­ã‚°
            props = torch.cuda.get_device_properties(device)
            logger.info(
                f"GPUæƒ…å ±: {props.name}, "
                f"ãƒ¡ãƒ¢ãƒª: {props.total_memory / 1024**3:.1f}GB, "
                f"SMã‚«ã‚¦ãƒ³ãƒˆ: {props.multi_processor_count}"
            )
        else:
            device = torch.device('cpu')
            logger.warning("GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPUã§å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™")
            
        return device
        
    async def process_batch_gpu(self, data: np.ndarray) -> np.ndarray:
        """
        ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’GPUã§é«˜é€Ÿå‡¦ç†
        
        æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆï¼š
        1. éåŒæœŸãƒ¡ãƒ¢ãƒªè»¢é€
        2. ã‚¹ãƒˆãƒªãƒ¼ãƒ ä¸¦åˆ—å‡¦ç†
        3. æ··åˆç²¾åº¦æ¼”ç®—
        4. ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã®æ´»ç”¨
        """
        # ãƒ‡ãƒ¼ã‚¿ã‚’GPUãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        with self.memory_pool:
            tensor = torch.from_numpy(data).to(
                self.device,
                non_blocking=True
            )
            
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å–å¾—
        stream = self.stream_pool.get_stream()
        
        with torch.cuda.stream(stream):
            # æ··åˆç²¾åº¦æ¼”ç®—ã§é«˜é€ŸåŒ–
            with torch.cuda.amp.autocast():
                # è¤‡é›‘ãªå‡¦ç†ã‚’å®Ÿè¡Œ
                result = await self._execute_gpu_computation(tensor)
                
            # çµæœã‚’CPUã«è»¢é€ï¼ˆéåŒæœŸï¼‰
            cpu_result = result.cpu()
            
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®åŒæœŸã‚’å¾…ã¤
        stream.synchronize()
        
        # NumPyé…åˆ—ã«å¤‰æ›ã—ã¦è¿”ã™
        return cpu_result.numpy()
        
    async def _execute_gpu_computation(self, tensor: torch.Tensor) -> torch.Tensor:
        """
        GPUä¸Šã§å®Ÿéš›ã®è¨ˆç®—ã‚’å®Ÿè¡Œ
        
        å®Ÿè£…ä¾‹ï¼šå¤§è¦æ¨¡ãªè¡Œåˆ—æ¼”ç®—ã¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¨è«–
        """
        batch_size = tensor.shape[0]
        
        # ä¸¦åˆ—å‡¦ç†ã®ãŸã‚ã«ãƒãƒƒãƒã‚’åˆ†å‰²
        if batch_size > 1000:
            # å¤§ããªãƒãƒƒãƒã¯åˆ†å‰²ã—ã¦å‡¦ç†
            chunk_size = 256
            chunks = tensor.split(chunk_size)
            
            # å„ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†
            results = []
            for chunk in chunks:
                # è¡Œåˆ—æ¼”ç®—
                intermediate = torch.matmul(chunk, self.weight_matrix)
                
                # æ´»æ€§åŒ–é–¢æ•°
                activated = torch.nn.functional.relu(intermediate)
                
                # æ­£è¦åŒ–
                normalized = torch.nn.functional.layer_norm(
                    activated,
                    normalized_shape=activated.shape[1:]
                )
                
                results.append(normalized)
                
            # çµæœã‚’çµåˆ
            result = torch.cat(results, dim=0)
        else:
            # å°ã•ãªãƒãƒƒãƒã¯ä¸€åº¦ã«å‡¦ç†
            result = self._process_single_batch(tensor)
            
        return result
        
    def optimize_gpu_memory(self):
        """
        GPU ãƒ¡ãƒ¢ãƒªã‚’æœ€é©åŒ–
        
        å®Ÿè¡Œã™ã‚‹æœ€é©åŒ–ï¼š
        1. æœªä½¿ç”¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        2. ãƒ¡ãƒ¢ãƒªã®æ–­ç‰‡åŒ–ã‚’è§£æ¶ˆ
        3. ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã®ã‚µã‚¤ã‚ºã‚’èª¿æ•´
        """
        if self.device.type == 'cuda':
            # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’è¨˜éŒ²
            before_memory = torch.cuda.memory_allocated(self.device)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
            torch.cuda.empty_cache()
            
            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¼·åˆ¶å®Ÿè¡Œ
            import gc
            gc.collect()
            
            # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚’æœ€é©åŒ–
            torch.cuda.set_per_process_memory_fraction(0.8)
            
            # æœ€é©åŒ–å¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³
            after_memory = torch.cuda.memory_allocated(self.device)
            freed_memory = (before_memory - after_memory) / 1024**2
            
            logger.info(f"GPU ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–å®Œäº†: {freed_memory:.1f}MB è§£æ”¾")
```

---

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è©³ç´°ãªè§£æ±ºç­–

#### å•é¡Œ1: ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®è¨ºæ–­ã¨ä¿®æ­£

ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã¯ã€é•·æ™‚é–“ç¨¼åƒã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§æ·±åˆ»ãªå•é¡Œã¨ãªã‚Šã¾ã™ï¼š

```python
class MemoryLeakDetector:
    """
    ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’æ¤œå‡ºã—ã€è‡ªå‹•çš„ã«ä¿®æ­£ã‚’è©¦ã¿ã‚‹
    """
    
    def __init__(self, threshold_mb: float = 100):
        self.threshold_mb = threshold_mb
        self.baseline_memory = None
        self.snapshots = []
        self.leak_patterns = defaultdict(list)
        
    async def start_monitoring(self, check_interval: int = 60):
        """
        ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã®ç¶™ç¶šçš„ãªç›£è¦–ã‚’é–‹å§‹
        """
        # ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é–‹å§‹
        tracemalloc.start()
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’è¨­å®š
        self.baseline_memory = self._get_current_memory_usage()
        
        while True:
            await asyncio.sleep(check_interval)
            
            # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯
            current_memory = self._get_current_memory_usage()
            memory_increase = current_memory - self.baseline_memory
            
            if memory_increase > self.threshold_mb:
                logger.warning(
                    f"ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§: {memory_increase:.1f}MB å¢—åŠ "
                )
                
                # è©³ç´°ãªåˆ†æã‚’å®Ÿè¡Œ
                leak_info = await self._analyze_memory_leak()
                
                # è‡ªå‹•ä¿®æ­£ã‚’è©¦ã¿ã‚‹
                if await self._attempt_automatic_fix(leak_info):
                    logger.info("ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®è‡ªå‹•ä¿®æ­£ã«æˆåŠŸ")
                else:
                    # ä¿®æ­£ã§ããªã„å ´åˆã¯è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
                    await self._generate_leak_report(leak_info)
                    
    def _get_current_memory_usage(self) -> float:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’MBå˜ä½ã§å–å¾—"""
        import psutil
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
        
    async def _analyze_memory_leak(self) -> Dict[str, Any]:
        """
        ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®è©³ç´°ãªåˆ†æ
        """
        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—
        snapshot = tracemalloc.take_snapshot()
        self.snapshots.append(snapshot)
        
        # å‰å›ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã¨æ¯”è¼ƒ
        if len(self.snapshots) > 1:
            prev_snapshot = self.snapshots[-2]
            stats = snapshot.compare_to(prev_snapshot, 'lineno')
            
            # ä¸Šä½ã®ãƒ¡ãƒ¢ãƒªå¢—åŠ ç®‡æ‰€ã‚’ç‰¹å®š
            leak_candidates = []
            for stat in stats[:10]:
                if stat.size_diff > 0:
                    leak_candidates.append({
                        'file': stat.traceback.format()[0],
                        'line': stat.lineno,
                        'size_diff': stat.size_diff,
                        'count_diff': stat.count_diff,
                        'traceback': stat.traceback.format()
                    })
                    
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
            for candidate in leak_candidates:
                key = f"{candidate['file']}:{candidate['line']}"
                self.leak_patterns[key].append({
                    'timestamp': datetime.now(),
                    'size_diff': candidate['size_diff']
                })
                
            return {
                'candidates': leak_candidates,
                'patterns': self._analyze_leak_patterns(),
                'total_increase': sum(c['size_diff'] for c in leak_candidates)
            }
            
        return {'candidates': [], 'patterns': {}, 'total_increase': 0}
        
    def _analyze_leak_patterns(self) -> Dict[str, Any]:
        """
        ãƒªãƒ¼ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã¦ã€ç¢ºå®Ÿãªãƒªãƒ¼ã‚¯ç®‡æ‰€ã‚’ç‰¹å®š
        """
        confirmed_leaks = {}
        
        for location, history in self.leak_patterns.items():
            if len(history) < 3:
                continue
                
            # é€£ç¶šçš„ãªå¢—åŠ ã‚’ãƒã‚§ãƒƒã‚¯
            consecutive_increases = 0
            total_increase = 0
            
            for i in range(1, len(history)):
                if history[i]['size_diff'] > 0:
                    consecutive_increases += 1
                    total_increase += history[i]['size_diff']
                else:
                    consecutive_increases = 0
                    
            if consecutive_increases >= 3:
                confirmed_leaks[location] = {
                    'confidence': min(consecutive_increases / 5, 1.0),
                    'total_leaked': total_increase,
                    'leak_rate': total_increase / len(history)
                }
                
        return confirmed_leaks
        
    async def _attempt_automatic_fix(self, leak_info: Dict[str, Any]) -> bool:
        """
        æ¤œå‡ºã•ã‚ŒãŸãƒªãƒ¼ã‚¯ã®è‡ªå‹•ä¿®æ­£ã‚’è©¦ã¿ã‚‹
        """
        fixed = False
        
        for candidate in leak_info['candidates']:
            location = f"{candidate['file']}:{candidate['line']}"
            
            # æ—¢çŸ¥ã®ãƒªãƒ¼ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            if 'cache' in location.lower():
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥é–¢é€£ã®ãƒªãƒ¼ã‚¯
                logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒªãƒ¼ã‚¯ã‚’æ¤œå‡º: {location}")
                self._fix_cache_leak()
                fixed = True
                
            elif 'connection' in location.lower() or 'socket' in location.lower():
                # æ¥ç¶šé–¢é€£ã®ãƒªãƒ¼ã‚¯
                logger.info(f"æ¥ç¶šãƒªãƒ¼ã‚¯ã‚’æ¤œå‡º: {location}")
                await self._fix_connection_leak()
                fixed = True
                
            elif 'list' in str(candidate['traceback']) or 'dict' in str(candidate['traceback']):
                # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³é–¢é€£ã®ãƒªãƒ¼ã‚¯
                logger.info(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒªãƒ¼ã‚¯ã‚’æ¤œå‡º: {location}")
                self._fix_collection_leak()
                fixed = True
                
        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¼·åˆ¶å®Ÿè¡Œ
        import gc
        gc.collect()
        
        return fixed
```

#### å•é¡Œ2: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ç‰¹å®š

```python
class PerformanceProfiler:
    """
    è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
    """
    
    def __init__(self):
        self.profiles = {}
        self.call_graph = defaultdict(list)
        self.slow_operations = []
        
    @contextmanager
    def profile_section(self, name: str):
        """
        ã‚³ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
        
        ä½¿ç”¨ä¾‹:
        with profiler.profile_section('database_query'):
            result = await db.query(sql)
        """
        start_time = time.perf_counter()
        start_memory = self._get_memory_usage()
        
        try:
            yield
        finally:
            end_time = time.perf_counter()
            end_memory = self._get_memory_usage()
            
            duration = end_time - start_time
            memory_delta = end_memory - start_memory
            
            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’è¨˜éŒ²
            if name not in self.profiles:
                self.profiles[name] = {
                    'count': 0,
                    'total_time': 0,
                    'min_time': float('inf'),
                    'max_time': 0,
                    'avg_time': 0,
                    'memory_impact': []
                }
                
            profile = self.profiles[name]
            profile['count'] += 1
            profile['total_time'] += duration
            profile['min_time'] = min(profile['min_time'], duration)
            profile['max_time'] = max(profile['max_time'], duration)
            profile['avg_time'] = profile['total_time'] / profile['count']
            profile['memory_impact'].append(memory_delta)
            
            # é…ã„æ“ä½œã‚’è¨˜éŒ²
            if duration > 1.0:  # 1ç§’ä»¥ä¸Š
                self.slow_operations.append({
                    'name': name,
                    'duration': duration,
                    'timestamp': datetime.now(),
                    'stack_trace': traceback.extract_stack()
                })
                
    async def analyze_bottlenecks(self) -> Dict[str, Any]:
        """
        ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’åˆ†æã—ã¦æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ
        """
        bottlenecks = []
        
        # æ™‚é–“çš„ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        for name, profile in self.profiles.items():
            if profile['avg_time'] > 0.1:  # 100msä»¥ä¸Š
                bottleneck = {
                    'type': 'time',
                    'name': name,
                    'severity': self._calculate_severity(profile['avg_time']),
                    'impact': profile['total_time'],
                    'suggestions': await self._generate_optimization_suggestions(
                        name, profile
                    )
                }
                bottlenecks.append(bottleneck)
                
        # ãƒ¡ãƒ¢ãƒªãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        for name, profile in self.profiles.items():
            avg_memory_impact = np.mean(profile['memory_impact'])
            if avg_memory_impact > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                bottleneck = {
                    'type': 'memory',
                    'name': name,
                    'severity': 'high' if avg_memory_impact > 100 * 1024 * 1024 else 'medium',
                    'impact': avg_memory_impact,
                    'suggestions': [
                        'ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ—ãƒ¼ãƒ«ã®ä½¿ç”¨ã‚’æ¤œè¨',
                        'ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®æœ€é©åŒ–',
                        'ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã¸ã®å¤‰æ›´'
                    ]
                }
                bottlenecks.append(bottleneck)
                
        return {
            'bottlenecks': sorted(
                bottlenecks,
                key=lambda x: x['impact'],
                reverse=True
            ),
            'total_operations': sum(p['count'] for p in self.profiles.values()),
            'slow_operations': self.slow_operations[-10:]  # æœ€æ–°10ä»¶
        }
        
    async def _generate_optimization_suggestions(
        self, 
        operation_name: str, 
        profile: Dict[str, Any]
    ) -> List[str]:
        """
        æ“ä½œã«åŸºã¥ã„ã¦æœ€é©åŒ–ã®ææ¡ˆã‚’ç”Ÿæˆ
        """
        suggestions = []
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£
        if 'database' in operation_name.lower() or 'query' in operation_name.lower():
            suggestions.extend([
                'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¿½åŠ ã‚’æ¤œè¨',
                'ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–ï¼ˆEXPLAIN ANALYZEã‚’å®Ÿè¡Œï¼‰',
                'ãƒãƒƒãƒå‡¦ç†ã®å°å…¥',
                'æ¥ç¶šãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®è¨­å®šã‚’ç¢ºèª'
            ])
            
        # APIé–¢é€£
        elif 'api' in operation_name.lower() or 'http' in operation_name.lower():
            suggestions.extend([
                'ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã®å®Ÿè£…',
                'ãƒãƒƒãƒAPIã®ä½¿ç”¨',
                'ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Ÿè£…',
                'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã®è¦‹ç›´ã—'
            ])
            
        # ãƒ•ã‚¡ã‚¤ãƒ«I/Oé–¢é€£
        elif 'file' in operation_name.lower() or 'disk' in operation_name.lower():
            suggestions.extend([
                'éåŒæœŸI/Oã®ä½¿ç”¨',
                'ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã®æœ€é©åŒ–',
                'ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨',
                'SSDã¸ã®ç§»è¡Œ'
            ])
            
        # ä¸€èˆ¬çš„ãªææ¡ˆ
        if profile['avg_time'] > 1.0:
            suggestions.append('å‡¦ç†ã®ä¸¦åˆ—åŒ–ã‚’æ¤œè¨')
        if profile['max_time'] / profile['avg_time'] > 10:
            suggestions.append('å¤–ã‚Œå€¤ã®åŸå› ã‚’èª¿æŸ»ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ãªã©ï¼‰')
            
        return suggestions
```

---

## ğŸš€ ä»Šå¾Œã®å±•æœ›

### 2025å¹´ä»¥é™ã®æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰

{short_title}ã®åˆ†é‡ã¯ã€ä»Šå¾Œã•ã‚‰ãªã‚‹é€²åŒ–ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ï¼š

#### 1. {keywords[2]}ã®å®Œå…¨è‡ªå‹•åŒ–

ç¾åœ¨ã¯äººé–“ã®ä»‹å…¥ãŒå¿…è¦ãªéƒ¨åˆ†ã‚‚ã€AIã®é€²åŒ–ã«ã‚ˆã‚Šå®Œå…¨è‡ªå‹•åŒ–ãŒå®Ÿç¾ã•ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚å…·ä½“çš„ã«ã¯ï¼š

- **è‡ªå·±ä¿®å¾©ã‚·ã‚¹ãƒ†ãƒ **: ã‚¨ãƒ©ãƒ¼ã‚’è‡ªå‹•çš„ã«æ¤œå‡ºã—ã€ä¿®æ­£æ¡ˆã‚’ç”Ÿæˆãƒ»é©ç”¨
- **è‡ªå‹•æœ€é©åŒ–**: ã‚·ã‚¹ãƒ†ãƒ ãŒè‡ªèº«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶™ç¶šçš„ã«æ”¹å–„
- **äºˆæ¸¬çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: è² è·ã‚’äºˆæ¸¬ã—ã¦äº‹å‰ã«ãƒªã‚½ãƒ¼ã‚¹ã‚’èª¿æ•´

#### 2. {keywords[3]}ã¨ã®æ·±ã„çµ±åˆ

ç•°ãªã‚‹æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯é–“ã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªé€£æºã«ã‚ˆã‚Šã€æ–°ãŸãªå¯èƒ½æ€§ãŒç”Ÿã¾ã‚Œã¾ã™ï¼š

- **ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«ãƒ—ãƒ­ãƒˆã‚³ãƒ«**: ç•°ãªã‚‹ã‚·ã‚¹ãƒ†ãƒ é–“ã®é€šä¿¡ã‚’æ¨™æº–åŒ–
- **ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**: AIãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã‚’æœ€é©åŒ–
- **è‡ªå‹•çµ±åˆ**: æ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹ã®è¿½åŠ ãŒè¨­å®šä¸è¦ã«

#### 3. é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨ã®èåˆ

é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®å®Ÿç”¨åŒ–ã«ã‚ˆã‚Šã€ã“ã‚Œã¾ã§ä¸å¯èƒ½ã ã£ãŸè¨ˆç®—ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ï¼š

- **è¶…é«˜é€Ÿæœ€é©åŒ–**: çµ„ã¿åˆã‚ã›æœ€é©åŒ–å•é¡Œã®ç¬æ™‚è§£æ±º
- **æš—å·æŠ€è¡“ã®é©æ–°**: é‡å­è€æ€§æš—å·ã®æ¨™æº–åŒ–
- **ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³èƒ½åŠ›**: è¤‡é›‘ãªã‚·ã‚¹ãƒ†ãƒ ã®å®Œå…¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

### ä»Šã™ãå§‹ã‚ã‚‰ã‚Œã‚‹æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **å®Ÿé¨“ç’°å¢ƒã®æ§‹ç¯‰**
   ```bash
   git clone {topic_data['source_url']}
   cd project-name
   docker-compose up -d
   ```

2. **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¸ã®å‚åŠ **
   - å…¬å¼ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ã§ã®æƒ…å ±äº¤æ›
   - ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®è²¢çŒ®
   - å‹‰å¼·ä¼šã‚„ãƒŸãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã¸ã®å‚åŠ 

3. **å®Ÿãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®é©ç”¨**
   - å°è¦æ¨¡ãªPoCã‹ã‚‰é–‹å§‹
   - æ®µéšçš„ãªæœ¬ç•ªç’°å¢ƒã¸ã®å°å…¥
   - ç¶™ç¶šçš„ãªæ”¹å–„ã‚µã‚¤ã‚¯ãƒ«ã®ç¢ºç«‹

---

## ğŸ“ ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ã€{title}ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¾ã—ãŸã€‚

### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã®æŒ¯ã‚Šè¿”ã‚Š

âœ… **{keywords[0]}ã®åŸºæœ¬æ¦‚å¿µã¨å®Ÿè£…æ–¹æ³•**
- ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ç†è§£
- å®Ÿè·µçš„ãªå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
- ã‚ˆãã‚ã‚‹è½ã¨ã—ç©´ã¨å›é¿æ–¹æ³•

âœ… **{keywords[1]}ã¨ã®åŠ¹æœçš„ãªé€£æºãƒ‘ã‚¿ãƒ¼ãƒ³**
- çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆ
- ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®æœ€é©åŒ–
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

âœ… **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**
- ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥
- ä¸¦åˆ—å‡¦ç†ã®æ´»ç”¨
- ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã®è‡ªå‹•åŒ–

âœ… **å®Ÿè·µçš„ãªãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ‰‹æ³•**
- å•é¡Œã®æ—©æœŸç™ºè¦‹
- æ ¹æœ¬åŸå› ã®åˆ†æ
- åŠ¹æœçš„ãªè§£æ±ºç­–ã®å®Ÿè£…

ã“ã‚Œã‚‰ã®çŸ¥è­˜ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã§é«˜å“è³ªãªã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚æŠ€è¡“ã¯æ—¥ã€…é€²åŒ–ã—ã¦ã„ã¾ã™ãŒã€æœ¬è¨˜äº‹ã§ç´¹ä»‹ã—ãŸåŸºæœ¬çš„ãªè€ƒãˆæ–¹ã¨å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€é•·æœŸçš„ã«å½¹ç«‹ã¤ã‚‚ã®ã§ã™ã€‚

### æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. æœ¬è¨˜äº‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿéš›ã«å‹•ã‹ã—ã¦ã¿ã‚‹
2. è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é©ç”¨ã§ãã‚‹éƒ¨åˆ†ã‚’ç‰¹å®šã™ã‚‹
3. å°ã•ãå§‹ã‚ã¦ã€æ®µéšçš„ã«æ‹¡å¼µã—ã¦ã„ã
4. çµæœã‚’æ¸¬å®šã—ã€ç¶™ç¶šçš„ã«æ”¹å–„ã™ã‚‹

### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨ãƒªã‚½ãƒ¼ã‚¹

- ğŸ“š [å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ]({topic_data['source_url']})
- ğŸ’¬ [ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ•ã‚©ãƒ¼ãƒ©ãƒ ]({random.choice(topic_data['reference_sites'])})
- ğŸ”§ [å®Ÿè£…ä¾‹ã¨ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰]({random.choice(topic_data['reference_sites'])})
- ğŸ“Š [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯]({random.choice(topic_data['reference_sites'])})

---

*ã“ã®è¨˜äº‹ãŒå½¹ã«ç«‹ã£ãŸã‚‰ã€ãœã²ã‚·ã‚§ã‚¢ã—ã¦ä»–ã®é–‹ç™ºè€…ã«ã‚‚åºƒã‚ã¦ãã ã•ã„ï¼*

*è³ªå•ã‚„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã€ã‚³ãƒ¡ãƒ³ãƒˆæ¬„ã§ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ã€‚ã¿ãªã•ã‚“ã®çµŒé¨“ã‚„çŸ¥è¦‹ã‚’å…±æœ‰ã—ã¦ã„ãŸã ã‘ã‚‹ã¨ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å…¨ä½“ã®æˆé•·ã«ã¤ãªãŒã‚Šã¾ã™ã€‚*

**åŸ·ç­†è€…ã«ã¤ã„ã¦**: Alic AI Blogã¯ã€æœ€æ–°ã®æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’24æ™‚é–“365æ—¥è‡ªå‹•çš„ã«åˆ†æã—ã€å®Ÿè·µçš„ãªæŠ€è¡“è¨˜äº‹ã‚’ç”Ÿæˆã™ã‚‹AIã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚äººé–“ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã¯è¿½ã„ãã‚Œãªã„é€Ÿåº¦ã§å¤‰åŒ–ã™ã‚‹æŠ€è¡“æƒ…å ±ã‚’ã€ã‚ã‹ã‚Šã‚„ã™ãæ•´ç†ã—ã¦ãŠå±Šã‘ã—ã¾ã™ã€‚

---
*ã“ã®è¨˜äº‹ã¯AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*
*Generated at {get_jst_now().strftime('%Y-%m-%d %H:%M:%S')} JST*
"""
    
    return content

def cleanup_old_articles(keep_count=5):
    """å¤ã„è¨˜äº‹ã‚’å‰Šé™¤ã—ã¦æœ€æ–°Nä»¶ã®ã¿ã‚’ä¿æŒ"""
    print(f"\nğŸ§¹ è¨˜äº‹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆæœ€æ–°{keep_count}ä»¶ã‚’ä¿æŒï¼‰")
    
    posts_dir = Path("posts")
    if not posts_dir.exists():
        return
    
    md_files = sorted(posts_dir.glob("*.md"), key=lambda x: x.stat().st_mtime, reverse=True)
    
    if len(md_files) <= keep_count:
        print(f"  ç¾åœ¨ã®è¨˜äº‹æ•°: {len(md_files)}ä»¶ - ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸è¦")
        return
    
    files_to_delete = md_files[keep_count:]
    print(f"  å‰Šé™¤å¯¾è±¡: {len(files_to_delete)}ä»¶ã®å¤ã„è¨˜äº‹")
    
    for md_file in files_to_delete:
        print(f"  ğŸ—‘ï¸  å‰Šé™¤: {md_file.name}")
        md_file.unlink()
        
        html_file = Path("docs/articles") / f"{md_file.stem}.html"
        if html_file.exists():
            html_file.unlink()
    
    print(f"  âœ… {len(files_to_delete)}ä»¶ã®è¨˜äº‹ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")

async def generate_single_article():
    """1ã¤ã®è¨˜äº‹ã‚’ç”Ÿæˆ"""
    
    blog_dir = Path(".")
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒˆãƒ”ãƒƒã‚¯ã‚’é¸æŠ
    topic_data = random.choice(TOPICS)
    jst_now = get_jst_now()
    article_id = f"article_{int(jst_now.timestamp())}"
    
    # è©³ç´°ãªè¨˜äº‹å†…å®¹ã‚’ç”Ÿæˆ
    content = generate_detailed_content(topic_data)
    
    # ã‚«ãƒ†ã‚´ãƒªãƒ¼æƒ…å ±ã‚’å–å¾—
    category = CATEGORIES[topic_data["category"]]
    
    # è¨˜äº‹ã‚’ä¿å­˜
    posts_dir = blog_dir / "posts"
    posts_dir.mkdir(exist_ok=True)
    
    article_path = posts_dir / f"{article_id}.md"
    with open(article_path, "w", encoding="utf-8") as f:
        f.write(f"---\n")
        f.write(f"title: {topic_data['title']}\n")
        f.write(f"date: {jst_now.strftime('%Y-%m-%d %H:%M')}\n")
        f.write(f"category: {category['name']}\n")
        f.write(f"tags: {', '.join(category['tags'])}\n")
        f.write(f"difficulty: {topic_data['difficulty']}\n")
        f.write(f"reading_time: {topic_data['reading_time']}\n")
        f.write(f"---\n\n")
        f.write(content)
    
    print(f"âœ… Generated: {topic_data['title']}")
    print(f"   ã‚«ãƒ†ã‚´ãƒªãƒ¼: {category['name']}")
    print(f"   ã‚¿ã‚°: {', '.join(category['tags'])}")
    print(f"   æ™‚åˆ»: {jst_now.strftime('%Y-%m-%d %H:%M:%S')} JST")
    
    # HTMLã«å¤‰æ›
    if Path("convert_articles_v3.py").exists():
        import subprocess
        print("ğŸ“ HTMLã«å¤‰æ›ä¸­...")
        result = subprocess.run(
            ["python", "convert_articles_v3.py"], 
            capture_output=True, 
            text=True
        )
        if result.returncode != 0:
            print(f"âŒ HTMLå¤‰æ›ã‚¨ãƒ©ãƒ¼: {result.stderr}")
        else:
            print("âœ… HTMLå¤‰æ›å®Œäº†")
    
    # index.htmlã‚’æ›´æ–°
    if Path("update_to_modern_ui_v3.py").exists():
        import subprocess
        print("ğŸ“ index.htmlã‚’æ›´æ–°ä¸­...")
        subprocess.run(["python", "update_to_modern_ui_v3.py"])
    
    return topic_data

async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ¤– Enhanced GitHub Actions Article Generator v3")
    print("=" * 50)
    
    jst_now = get_jst_now()
    print(f"â° ç¾åœ¨ã®æ—¥æœ¬æ™‚é–“: {jst_now.strftime('%Y-%m-%d %H:%M:%S')} JST")
    
    # 1ã¤ã®è¨˜äº‹ã‚’ç”Ÿæˆ
    topic = await generate_single_article()
    
    # å¤ã„è¨˜äº‹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_old_articles(keep_count=5)
    
    # HTMLã‚’å†ç”Ÿæˆ
    if Path("convert_articles_v3.py").exists():
        print("\nğŸ“„ HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†ç”Ÿæˆã—ã¦ã„ã¾ã™...")
        os.system("python convert_articles_v3.py")
    
    # index.htmlã‚’æ›´æ–°
    if Path("update_to_modern_ui_v3.py").exists():
        print("ğŸ“ index.htmlã‚’æœ€çµ‚æ›´æ–°ä¸­...")
        os.system("python update_to_modern_ui_v3.py")
    
    print(f"\nâœ… Successfully generated article: {topic['title']}")
    print(f"âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº† - æœ€æ–°5è¨˜äº‹ã‚’ä¿æŒ")

if __name__ == "__main__":
    asyncio.run(main())
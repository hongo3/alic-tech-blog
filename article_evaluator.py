#!/usr/bin/env python3
"""
è¨˜äº‹è©•ä¾¡ã¨è‡ªå·±æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 
å‰å›ã®è¨˜äº‹ã¨ãƒ«ãƒ¼ãƒ«ã‚’è©•ä¾¡ã—ã€ç¶™ç¶šçš„ã«å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹
"""

import json
import re
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Tuple
import asyncio

# æ—¥æœ¬æ¨™æº–æ™‚ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³
JST = timezone(timedelta(hours=9))

class ArticleEvaluator:
    """è¨˜äº‹ã®è©•ä¾¡ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.evaluation_history_file = Path("evaluation_history.json")
        self.rules_file = Path("BLOG_WRITING_RULES.md")
        self.load_history()
        
    def load_history(self):
        """è©•ä¾¡å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€"""
        if self.evaluation_history_file.exists():
            with open(self.evaluation_history_file, "r", encoding="utf-8") as f:
                self.history = json.load(f)
        else:
            self.history = {
                "evaluations": [],
                "rule_updates": [],
                "quality_trends": {}
            }
    
    def save_history(self):
        """è©•ä¾¡å±¥æ­´ã‚’ä¿å­˜"""
        with open(self.evaluation_history_file, "w", encoding="utf-8") as f:
            json.dump(self.history, f, ensure_ascii=False, indent=2)
    
    async def evaluate_article(self, article_path: Path) -> Dict[str, Any]:
        """è¨˜äº‹ã‚’è©•ä¾¡ã™ã‚‹"""
        
        with open(article_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        metadata = self._extract_metadata(content)
        
        # è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        scores = {
            "technical_accuracy": await self._evaluate_technical_accuracy(content, metadata),
            "readability": await self._evaluate_readability(content, metadata),
            "practicality": await self._evaluate_practicality(content, metadata),
            "originality": await self._evaluate_originality(content, metadata)
        }
        
        # è©³ç´°ãªè©•ä¾¡çµæœ
        evaluation = {
            "article_path": str(article_path),
            "timestamp": datetime.now(JST).isoformat(),
            "metadata": metadata,
            "scores": scores,
            "total_score": sum(scores.values()),
            "strengths": [],
            "weaknesses": [],
            "improvement_suggestions": []
        }
        
        # å¼·ã¿ã¨å¼±ã¿ã‚’åˆ†æ
        evaluation.update(self._analyze_strengths_weaknesses(content, scores))
        
        return evaluation
    
    def _extract_metadata(self, content: str) -> Dict[str, str]:
        """è¨˜äº‹ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        metadata = {}
        
        if content.startswith("---"):
            parts = content.split("---", 2)
            if len(parts) >= 2:
                meta_section = parts[1].strip()
                for line in meta_section.split("\n"):
                    if ":" in line:
                        key, value = line.split(":", 1)
                        metadata[key.strip()] = value.strip()
        
        return metadata
    
    async def _evaluate_technical_accuracy(self, content: str, metadata: Dict[str, str]) -> float:
        """æŠ€è¡“çš„æ­£ç¢ºæ€§ã‚’è©•ä¾¡ï¼ˆ25ç‚¹æº€ç‚¹ï¼‰"""
        score = 25.0
        
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å­˜åœ¨ã¨å“è³ªã‚’ãƒã‚§ãƒƒã‚¯
        code_blocks = re.findall(r'```[\w]*\n(.*?)\n```', content, re.DOTALL)
        
        if len(code_blocks) == 0:
            score -= 10  # ã‚³ãƒ¼ãƒ‰ãŒãªã„
        else:
            # ã‚³ãƒ¼ãƒ‰ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯
            for code in code_blocks:
                # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æœ‰ç„¡
                if 'try:' in code or 'except' in code or 'error' in code.lower():
                    score -= 0  # è‰¯ã„
                else:
                    score -= 0.5  # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãªã—
                
                # ã‚³ãƒ¡ãƒ³ãƒˆã®æœ‰ç„¡
                if '#' in code and not code.strip().startswith('#'):
                    score -= 0  # ã‚³ãƒ¡ãƒ³ãƒˆã‚ã‚Š
                else:
                    score -= 0.5  # ã‚³ãƒ¡ãƒ³ãƒˆãªã—
        
        # æŠ€è¡“ç”¨èªã®é©åˆ‡ãªä½¿ç”¨
        tech_terms = ["API", "async", "await", "class", "function", "database", "cache"]
        term_count = sum(1 for term in tech_terms if term.lower() in content.lower())
        if term_count < 5:
            score -= 2  # æŠ€è¡“ç”¨èªãŒå°‘ãªã„
        
        # æœ€æ–°æ€§ã®ãƒã‚§ãƒƒã‚¯ï¼ˆ2025å¹´ã®è¨˜è¿°ãŒã‚ã‚‹ã‹ï¼‰
        if "2025" in content or "æœ€æ–°" in content:
            score -= 0  # æœ€æ–°æ€§ã‚ã‚Š
        else:
            score -= 1
        
        return max(0, score)
    
    async def _evaluate_readability(self, content: str, metadata: Dict[str, str]) -> float:
        """èª­ã¿ã‚„ã™ã•ã‚’è©•ä¾¡ï¼ˆ25ç‚¹æº€ç‚¹ï¼‰"""
        score = 25.0
        
        # æ–‡ç« æ§‹é€ ã®ãƒã‚§ãƒƒã‚¯
        sections = re.findall(r'^#{1,3}\s+(.+)$', content, re.MULTILINE)
        if len(sections) < 5:
            score -= 3  # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå°‘ãªã„
        
        # æ®µè½ã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯
        paragraphs = content.split('\n\n')
        long_paragraphs = [p for p in paragraphs if len(p) > 500 and not p.startswith('```')]
        if len(long_paragraphs) > 3:
            score -= 2  # é•·ã™ãã‚‹æ®µè½ãŒå¤šã„
        
        # ç®‡æ¡æ›¸ãã®ä½¿ç”¨
        bullet_points = content.count('\n- ') + content.count('\n* ') + content.count('\n1. ')
        if bullet_points < 5:
            score -= 2  # ç®‡æ¡æ›¸ããŒå°‘ãªã„
        
        # å°‚é–€ç”¨èªã®èª¬æ˜
        if "ã¨ã¯" in content or "ã«ã¤ã„ã¦" in content or "ã®æ¦‚è¦" in content:
            score -= 0  # èª¬æ˜ã‚ã‚Š
        else:
            score -= 3  # èª¬æ˜ä¸è¶³
        
        # AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å­˜åœ¨
        if "ãªãœã“ã®è¨˜äº‹ã‚’æ›¸ã“ã†ã¨æ€ã£ãŸã®ã‹" in content or "AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹" in content:
            score -= 0
        else:
            score -= 5  # å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒãªã„
        
        return max(0, score)
    
    async def _evaluate_practicality(self, content: str, metadata: Dict[str, str]) -> float:
        """å®Ÿç”¨æ€§ã‚’è©•ä¾¡ï¼ˆ25ç‚¹æº€ç‚¹ï¼‰"""
        score = 25.0
        
        # å®Ÿè£…ä¾‹ã®å­˜åœ¨
        if "å®Ÿè£…ä¾‹" in content or "ä½¿ç”¨ä¾‹" in content or "ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰" in content:
            score -= 0
        else:
            score -= 5
        
        # ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ã®å­˜åœ¨
        if "Step 1" in content or "ã‚¹ãƒ†ãƒƒãƒ—1" in content or "æ‰‹é †" in content:
            score -= 0
        else:
            score -= 3
        
        # ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if "ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°" in content or "ã‚ˆãã‚ã‚‹å•é¡Œ" in content or "ã‚¨ãƒ©ãƒ¼" in content:
            score -= 0
        else:
            score -= 2
        
        # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«/ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †
        if "pip install" in content or "npm install" in content or "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«" in content:
            score -= 0
        else:
            score -= 2
        
        # å®Ÿè¡Œå¯èƒ½ãªã‚³ãƒ¼ãƒ‰ä¾‹ã®æ•°
        executable_patterns = ["if __name__", "async def main", "def main(", "asyncio.run("]
        executable_count = sum(1 for pattern in executable_patterns if pattern in content)
        if executable_count < 2:
            score -= 3
        
        return max(0, score)
    
    async def _evaluate_originality(self, content: str, metadata: Dict[str, str]) -> float:
        """ç‹¬è‡ªæ€§ã‚’è©•ä¾¡ï¼ˆ25ç‚¹æº€ç‚¹ï¼‰"""
        score = 25.0
        
        # è¤‡æ•°æŠ€è¡“ã®çµ„ã¿åˆã‚ã›
        tech_combinations = 0
        if "ã¨ã®é€£æº" in content or "ã‚’çµ„ã¿åˆã‚ã›" in content or "çµ±åˆ" in content:
            tech_combinations += 1
        if tech_combinations == 0:
            score -= 3
        
        # AIãªã‚‰ã§ã¯ã®è¦–ç‚¹
        if "AIã®è¦–ç‚¹" in content or "è‡ªå‹•åŒ–" in content or "æ©Ÿæ¢°å­¦ç¿’" in content:
            score -= 0
        else:
            score -= 3
        
        # è©³ç´°ãªå®Ÿè£…èª¬æ˜
        detailed_explanations = content.count("è©³ã—ã") + content.count("è©³ç´°ã«") + content.count("æ·±ã")
        if detailed_explanations < 3:
            score -= 2
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®è¨€åŠ
        if "æœ€é©åŒ–" in content or "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹" in content or "é«˜é€ŸåŒ–" in content:
            score -= 0
        else:
            score -= 2
        
        # æ–‡ç« é‡ï¼ˆç‹¬è‡ªã®è©³ç´°ãªèª¬æ˜ãŒã‚ã‚‹ã‹ï¼‰
        word_count = len(content.replace(" ", "").replace("\n", ""))
        if word_count < 5000:
            score -= 5  # æ–‡ç« ãŒçŸ­ã™ãã‚‹
        
        return max(0, score)
    
    def _analyze_strengths_weaknesses(self, content: str, scores: Dict[str, float]) -> Dict[str, List[str]]:
        """å¼·ã¿ã¨å¼±ã¿ã‚’åˆ†æ"""
        result = {
            "strengths": [],
            "weaknesses": [],
            "improvement_suggestions": []
        }
        
        # ã‚¹ã‚³ã‚¢ã«åŸºã¥ãåˆ†æ
        for category, score in scores.items():
            if score >= 20:
                result["strengths"].append(f"{category}: å„ªç§€ãªã‚¹ã‚³ã‚¢ ({score:.1f}/25)")
            elif score < 15:
                result["weaknesses"].append(f"{category}: æ”¹å–„ãŒå¿…è¦ ({score:.1f}/25)")
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ
        if len(re.findall(r'```[\w]*\n', content)) > 5:
            result["strengths"].append("è±Šå¯Œãªã‚³ãƒ¼ãƒ‰ä¾‹")
        else:
            result["improvement_suggestions"].append("ã‚ˆã‚Šå¤šãã®ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’è¿½åŠ ")
        
        if "ãªãœã“ã®è¨˜äº‹ã‚’æ›¸ã“ã†ã¨æ€ã£ãŸã®ã‹" in content:
            result["strengths"].append("AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ãŒæ˜ç¢º")
        else:
            result["weaknesses"].append("AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒä¸è¶³")
            result["improvement_suggestions"].append("è¨˜äº‹å†’é ­ã«AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¿½åŠ ")
        
        # æ–‡ç« ã¨ã‚³ãƒ¼ãƒ‰ã®ãƒãƒ©ãƒ³ã‚¹
        code_ratio = len(re.findall(r'```[\w]*\n(.*?)\n```', content, re.DOTALL)) / max(1, content.count('\n\n'))
        if 0.2 <= code_ratio <= 0.4:
            result["strengths"].append("æ–‡ç« ã¨ã‚³ãƒ¼ãƒ‰ã®è‰¯å¥½ãªãƒãƒ©ãƒ³ã‚¹")
        else:
            result["improvement_suggestions"].append("æ–‡ç« ã¨ã‚³ãƒ¼ãƒ‰ã®ãƒãƒ©ãƒ³ã‚¹ã‚’èª¿æ•´ï¼ˆç›®æ¨™: 70:30ï¼‰")
        
        return result

class RuleEvaluator:
    """ãƒ«ãƒ¼ãƒ«è‡ªä½“ã‚’è©•ä¾¡ã—ã€æ›´æ–°ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.rules_file = Path("BLOG_WRITING_RULES.md")
        self.rules_history_file = Path("rules_history.json")
        self.load_rules_history()
    
    def load_rules_history(self):
        """ãƒ«ãƒ¼ãƒ«æ›´æ–°å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€"""
        if self.rules_history_file.exists():
            with open(self.rules_history_file, "r", encoding="utf-8") as f:
                self.rules_history = json.load(f)
        else:
            self.rules_history = {
                "versions": [],
                "update_reasons": []
            }
    
    def save_rules_history(self):
        """ãƒ«ãƒ¼ãƒ«æ›´æ–°å±¥æ­´ã‚’ä¿å­˜"""
        with open(self.rules_history_file, "w", encoding="utf-8") as f:
            json.dump(self.rules_history, f, ensure_ascii=False, indent=2)
    
    async def evaluate_rules(self, recent_evaluations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æœ€è¿‘ã®è©•ä¾¡çµæœã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒ«ã‚’è©•ä¾¡"""
        
        rule_evaluation = {
            "timestamp": datetime.now(JST).isoformat(),
            "current_effectiveness": 0,
            "suggested_updates": [],
            "rationale": []
        }
        
        if not recent_evaluations:
            return rule_evaluation
        
        # æœ€è¿‘ã®è¨˜äº‹ã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        avg_scores = {
            "technical_accuracy": 0,
            "readability": 0,
            "practicality": 0,
            "originality": 0
        }
        
        for eval in recent_evaluations:
            for key in avg_scores:
                avg_scores[key] += eval["scores"][key]
        
        for key in avg_scores:
            avg_scores[key] /= len(recent_evaluations)
        
        total_avg = sum(avg_scores.values())
        rule_evaluation["current_effectiveness"] = total_avg
        
        # ãƒ«ãƒ¼ãƒ«æ›´æ–°ã®ææ¡ˆ
        if avg_scores["readability"] < 18:
            rule_evaluation["suggested_updates"].append({
                "rule": "èª­ã¿ã‚„ã™ã•ã®åŸºæº–ã‚’å¼·åŒ–",
                "action": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®ãƒãƒ¼ã‚¸ãƒ³ã‚’å¢—ã‚„ã—ã€èª¬æ˜ã‚’ã‚ˆã‚Šè©³ç´°ã«",
                "priority": "high"
            })
            rule_evaluation["rationale"].append("æœ€è¿‘ã®è¨˜äº‹ã§èª­ã¿ã‚„ã™ã•ã‚¹ã‚³ã‚¢ãŒä½ã„")
        
        if avg_scores["originality"] < 18:
            rule_evaluation["suggested_updates"].append({
                "rule": "ç‹¬è‡ªæ€§ã®è¦ä»¶ã‚’æ›´æ–°",
                "action": "è¤‡æ•°æŠ€è¡“ã®çµ„ã¿åˆã‚ã›ã‚’å¿…é ˆã«ã€ã‚ˆã‚Šæ·±ã„åˆ†æã‚’è¦æ±‚",
                "priority": "medium"
            })
            rule_evaluation["rationale"].append("ç‹¬è‡ªæ€§ã‚¹ã‚³ã‚¢ãŒç›®æ¨™ã‚’ä¸‹å›ã£ã¦ã„ã‚‹")
        
        # åŒã˜å•é¡ŒãŒç¹°ã‚Šè¿”ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        common_weaknesses = {}
        for eval in recent_evaluations:
            for weakness in eval.get("weaknesses", []):
                common_weaknesses[weakness] = common_weaknesses.get(weakness, 0) + 1
        
        for weakness, count in common_weaknesses.items():
            if count >= 3:  # 3è¨˜äº‹ä»¥ä¸Šã§åŒã˜å•é¡Œ
                rule_evaluation["suggested_updates"].append({
                    "rule": f"{weakness}ã«å¯¾ã™ã‚‹ãƒ«ãƒ¼ãƒ«è¿½åŠ ",
                    "action": "æ˜ç¢ºãªåŸºæº–ã¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’è¿½åŠ ",
                    "priority": "high"
                })
                rule_evaluation["rationale"].append(f"{weakness}ãŒ{count}è¨˜äº‹ã§ç™ºç”Ÿ")
        
        return rule_evaluation
    
    async def update_rules(self, rule_evaluation: Dict[str, Any]) -> bool:
        """ãƒ«ãƒ¼ãƒ«ã‚’æ›´æ–°ã™ã‚‹"""
        
        if not rule_evaluation["suggested_updates"]:
            return False
        
        # é«˜å„ªå…ˆåº¦ã®æ›´æ–°ã®ã¿ã‚’é©ç”¨
        high_priority_updates = [
            update for update in rule_evaluation["suggested_updates"] 
            if update["priority"] == "high"
        ]
        
        if not high_priority_updates:
            return False
        
        # ç¾åœ¨ã®ãƒ«ãƒ¼ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        with open(self.rules_file, "r", encoding="utf-8") as f:
            current_rules = f.read()
        
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’æ›´æ–°
        version_match = re.search(r'### v([\d.]+)', current_rules)
        if version_match:
            current_version = version_match.group(1)
            new_version = self._increment_version(current_version)
        else:
            new_version = "1.1"
        
        # æ›´æ–°å†…å®¹ã‚’è¿½åŠ 
        update_date = datetime.now(JST).strftime('%Y-%m-%d')
        update_section = f"\n### v{new_version} ({update_date})\n"
        
        for update in high_priority_updates:
            update_section += f"- {update['rule']}: {update['action']}\n"
        
        # ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        updated_rules = re.sub(
            r'(### ä»Šå¾Œã®æ›´æ–°äºˆå®š)',
            update_section + r'\n\1',
            current_rules
        )
        
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’æ›´æ–°
        updated_rules = re.sub(
            r'\*ãƒãƒ¼ã‚¸ãƒ§ãƒ³: [\d.]+\*',
            f'*ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {new_version}*',
            updated_rules
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€
        with open(self.rules_file, "w", encoding="utf-8") as f:
            f.write(updated_rules)
        
        # å±¥æ­´ã‚’æ›´æ–°
        self.rules_history["versions"].append({
            "version": new_version,
            "date": update_date,
            "updates": high_priority_updates,
            "rationale": rule_evaluation["rationale"]
        })
        self.save_rules_history()
        
        return True
    
    def _increment_version(self, version: str) -> str:
        """ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ"""
        parts = version.split('.')
        if len(parts) == 2:
            major, minor = parts
            return f"{major}.{int(minor) + 1}"
        return "1.1"

class SelfImprovingBlogSystem:
    """è‡ªå·±æ”¹å–„å‹ãƒ–ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.article_evaluator = ArticleEvaluator()
        self.rule_evaluator = RuleEvaluator()
    
    async def evaluate_and_improve(self) -> Dict[str, Any]:
        """è©•ä¾¡ã¨æ”¹å–„ã®ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹"""
        
        # æœ€æ–°ã®è¨˜äº‹ã‚’è©•ä¾¡
        posts_dir = Path("posts")
        recent_articles = sorted(
            posts_dir.glob("*.md"), 
            key=lambda x: x.stat().st_mtime, 
            reverse=True
        )[:5]  # æœ€æ–°5è¨˜äº‹
        
        evaluations = []
        for article in recent_articles:
            evaluation = await self.article_evaluator.evaluate_article(article)
            evaluations.append(evaluation)
            
            # è©•ä¾¡å±¥æ­´ã«è¿½åŠ 
            self.article_evaluator.history["evaluations"].append(evaluation)
        
        # å±¥æ­´ã‚’ä¿å­˜
        self.article_evaluator.save_history()
        
        # ãƒ«ãƒ¼ãƒ«ã‚’è©•ä¾¡
        rule_evaluation = await self.rule_evaluator.evaluate_rules(evaluations)
        
        # ãƒ«ãƒ¼ãƒ«ã‚’æ›´æ–°
        rules_updated = await self.rule_evaluator.update_rules(rule_evaluation)
        
        # æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        improvement_report = {
            "timestamp": datetime.now(JST).isoformat(),
            "articles_evaluated": len(evaluations),
            "average_score": sum(e["total_score"] for e in evaluations) / len(evaluations) if evaluations else 0,
            "rules_updated": rules_updated,
            "rule_evaluation": rule_evaluation,
            "recent_evaluations": evaluations[-3:],  # æœ€æ–°3ä»¶ã®è©•ä¾¡
            "improvement_trends": self._analyze_trends()
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
        report_file = Path(f"improvement_report_{datetime.now(JST).strftime('%Y%m%d_%H%M%S')}.json")
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(improvement_report, f, ensure_ascii=False, indent=2)
        
        return improvement_report
    
    def _analyze_trends(self) -> Dict[str, Any]:
        """å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ"""
        evaluations = self.article_evaluator.history.get("evaluations", [])
        
        if len(evaluations) < 2:
            return {"trend": "insufficient_data"}
        
        # æœ€æ–°10ä»¶ã¨éå»10ä»¶ã‚’æ¯”è¼ƒ
        recent = evaluations[-10:]
        older = evaluations[-20:-10] if len(evaluations) >= 20 else evaluations[:len(evaluations)//2]
        
        recent_avg = sum(e["total_score"] for e in recent) / len(recent)
        older_avg = sum(e["total_score"] for e in older) / len(older) if older else recent_avg
        
        improvement = recent_avg - older_avg
        
        return {
            "trend": "improving" if improvement > 0 else "declining",
            "improvement_rate": improvement,
            "recent_average": recent_avg,
            "historical_average": older_avg,
            "evaluation_count": len(evaluations)
        }

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”„ è‡ªå·±æ”¹å–„å‹ãƒ–ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ  - è©•ä¾¡ã¨æ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹")
    print("=" * 60)
    
    system = SelfImprovingBlogSystem()
    
    # è©•ä¾¡ã¨æ”¹å–„ã‚’å®Ÿè¡Œ
    report = await system.evaluate_and_improve()
    
    print(f"\nğŸ“Š è©•ä¾¡å®Œäº†:")
    print(f"  - è©•ä¾¡è¨˜äº‹æ•°: {report['articles_evaluated']}")
    print(f"  - å¹³å‡ã‚¹ã‚³ã‚¢: {report['average_score']:.1f}/100")
    print(f"  - ãƒ«ãƒ¼ãƒ«æ›´æ–°: {'å®Ÿæ–½' if report['rules_updated'] else 'ä¸è¦'}")
    
    if report["improvement_trends"]["trend"] != "insufficient_data":
        print(f"\nğŸ“ˆ å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰: {report['improvement_trends']['trend']}")
        print(f"  - æ”¹å–„ç‡: {report['improvement_trends']['improvement_rate']:.1f}ãƒã‚¤ãƒ³ãƒˆ")
    
    print("\nâœ… è©•ä¾¡ã¨æ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†")
    print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: improvement_report_*.json")

if __name__ == "__main__":
    asyncio.run(main())